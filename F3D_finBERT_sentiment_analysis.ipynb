{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3D: BERT Model 4 - BERT sentiment analysis using finBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Required Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import required libraries\n",
    "*Ignore the warning about audio backend since it is not required for our purpose*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karthik\\AppData\\Roaming\\Python\\Python38\\site-packages\\torchaudio\\backend\\utils.py:67: UserWarning: No audio backend is available.\n",
      "  warnings.warn('No audio backend is available.')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import reddit and twitter data for which we have to populate the sentiments\n",
    "- *we will use them on the model after the model is trained with the training dataset from kaggle*\n",
    "- *we save data in two dictionaries; one for reddit and one for twitter to make it more organized*### 1.3 Import reddit and twitter data for which we have to populate the sentiments\n",
    "- *we will use them on the model after the model is trained with the training dataset from kaggle*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick_list =  ['AAPL', 'AMC', 'DKNG', 'TSLA', 'AMD', 'BABA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df_dict = {tick: pd.read_pickle(f\"C:\\\\Users\\\\Karthik\\\\Desktop\\\\Dissertation\\\\Reddit\\\\consolidated_pickle_files\\\\reddit_{tick}_df_for_BERT.pkl\") for tick in tick_list}\n",
    "# reddit_df_dict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>top</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>ticker</th>\n",
       "      <th>YearMonDay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Non-troll post. I started 2 weeks ago and have...</td>\n",
       "      <td>1609500199</td>\n",
       "      <td>ghp72zs</td>\n",
       "      <td>top</td>\n",
       "      <td>21</td>\n",
       "      <td>Jan</td>\n",
       "      <td>01</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>21Jan01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>we eat cornbread on new years day to ensure a ...</td>\n",
       "      <td>1609524808</td>\n",
       "      <td>ghqo6qb</td>\n",
       "      <td>top</td>\n",
       "      <td>21</td>\n",
       "      <td>Jan</td>\n",
       "      <td>01</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>21Jan01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TSLA 850 EOD</td>\n",
       "      <td>1612203941</td>\n",
       "      <td>glmp0gv</td>\n",
       "      <td>top</td>\n",
       "      <td>21</td>\n",
       "      <td>Feb</td>\n",
       "      <td>01</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>21Feb01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>joe weisenthal is the ultimate chad( who else ...</td>\n",
       "      <td>1612207510</td>\n",
       "      <td>glmyg66</td>\n",
       "      <td>top</td>\n",
       "      <td>21</td>\n",
       "      <td>Feb</td>\n",
       "      <td>01</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>21Feb01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TSLA ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€</td>\n",
       "      <td>1612211972</td>\n",
       "      <td>gln9v98</td>\n",
       "      <td>top</td>\n",
       "      <td>21</td>\n",
       "      <td>Feb</td>\n",
       "      <td>01</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>21Feb01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15125</th>\n",
       "      <td>TSLA Drill Team 6 reporting for duty</td>\n",
       "      <td>1609359381</td>\n",
       "      <td>ghj65jr</td>\n",
       "      <td>top</td>\n",
       "      <td>20</td>\n",
       "      <td>Dec</td>\n",
       "      <td>30</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>20Dec30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15126</th>\n",
       "      <td>TSLAâ€™s still having them TSLA days I see. Join...</td>\n",
       "      <td>1609359393</td>\n",
       "      <td>ghj66f1</td>\n",
       "      <td>top</td>\n",
       "      <td>20</td>\n",
       "      <td>Dec</td>\n",
       "      <td>30</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>20Dec30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15127</th>\n",
       "      <td>TSLA ðŸŽ° EOY coming</td>\n",
       "      <td>1609360555</td>\n",
       "      <td>ghj8jam</td>\n",
       "      <td>top</td>\n",
       "      <td>20</td>\n",
       "      <td>Dec</td>\n",
       "      <td>30</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>20Dec30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15128</th>\n",
       "      <td>Thank fuck I didn't dump my TSLA calls yesterday</td>\n",
       "      <td>1609361591</td>\n",
       "      <td>ghjamup</td>\n",
       "      <td>top</td>\n",
       "      <td>20</td>\n",
       "      <td>Dec</td>\n",
       "      <td>30</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>20Dec30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15129</th>\n",
       "      <td>What date is TSLA announcing delivery numbers?</td>\n",
       "      <td>1609361685</td>\n",
       "      <td>ghjatox</td>\n",
       "      <td>top</td>\n",
       "      <td>20</td>\n",
       "      <td>Dec</td>\n",
       "      <td>30</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>20Dec30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15130 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    body  created_utc  \\\n",
       "0      Non-troll post. I started 2 weeks ago and have...   1609500199   \n",
       "1      we eat cornbread on new years day to ensure a ...   1609524808   \n",
       "2                                           TSLA 850 EOD   1612203941   \n",
       "3      joe weisenthal is the ultimate chad( who else ...   1612207510   \n",
       "4                                             TSLA ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€   1612211972   \n",
       "...                                                  ...          ...   \n",
       "15125               TSLA Drill Team 6 reporting for duty   1609359381   \n",
       "15126  TSLAâ€™s still having them TSLA days I see. Join...   1609359393   \n",
       "15127                                  TSLA ðŸŽ° EOY coming   1609360555   \n",
       "15128   Thank fuck I didn't dump my TSLA calls yesterday   1609361591   \n",
       "15129     What date is TSLA announcing delivery numbers?   1609361685   \n",
       "\n",
       "            id  top year month day ticker YearMonDay  \n",
       "0      ghp72zs  top   21   Jan  01   TSLA    21Jan01  \n",
       "1      ghqo6qb  top   21   Jan  01   TSLA    21Jan01  \n",
       "2      glmp0gv  top   21   Feb  01   TSLA    21Feb01  \n",
       "3      glmyg66  top   21   Feb  01   TSLA    21Feb01  \n",
       "4      gln9v98  top   21   Feb  01   TSLA    21Feb01  \n",
       "...        ...  ...  ...   ...  ..    ...        ...  \n",
       "15125  ghj65jr  top   20   Dec  30   TSLA    20Dec30  \n",
       "15126  ghj66f1  top   20   Dec  30   TSLA    20Dec30  \n",
       "15127  ghj8jam  top   20   Dec  30   TSLA    20Dec30  \n",
       "15128  ghjamup  top   20   Dec  30   TSLA    20Dec30  \n",
       "15129  ghjatox  top   20   Dec  30   TSLA    20Dec30  \n",
       "\n",
       "[15130 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df_dict['TSLA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df_dict = {tick: pd.read_pickle(f\"C:\\\\Users\\\\Karthik\\\\Desktop\\\\Dissertation\\\\Twitter\\\\consolidated_pickle_files\\\\twitter_{tick}_df_for_BERT.pkl\") for tick in tick_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>lang</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>ticker</th>\n",
       "      <th>YearMonDay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-09-18 19:56:59+00:00</td>\n",
       "      <td>@The_RockTrading Bullish on $TSLA this week &amp;a...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>Bullish on $TSLA this week &amp; $Aapl</td>\n",
       "      <td>21</td>\n",
       "      <td>Sep</td>\n",
       "      <td>18</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>21Sep18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-09-18 04:16:52+00:00</td>\n",
       "      <td>$TSLA now is the same as $aapl in the 80s ! @e...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>$TSLA now is the same as $aapl in the 80s !</td>\n",
       "      <td>21</td>\n",
       "      <td>Sep</td>\n",
       "      <td>18</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>21Sep18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-09-16 19:01:28+00:00</td>\n",
       "      <td>Added more $TSLA and $aapl to long, because I ...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>Added more $TSLA and $aapl to long, because I ...</td>\n",
       "      <td>21</td>\n",
       "      <td>Sep</td>\n",
       "      <td>16</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>21Sep16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-09-16 15:23:53+00:00</td>\n",
       "      <td>@NeilRog49855230 @Gays4Tesla @TheMaverickWS Th...</td>\n",
       "      <td>3</td>\n",
       "      <td>en</td>\n",
       "      <td>There is plenty of information available on-li...</td>\n",
       "      <td>21</td>\n",
       "      <td>Sep</td>\n",
       "      <td>16</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>21Sep16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-09-16 15:23:11+00:00</td>\n",
       "      <td>There is plenty of information available on-li...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>There is plenty of information available on-li...</td>\n",
       "      <td>21</td>\n",
       "      <td>Sep</td>\n",
       "      <td>16</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>21Sep16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179489</th>\n",
       "      <td>2020-06-01 18:42:22+00:00</td>\n",
       "      <td>Normally volatile Tesla $TSLA is the IBD Stock...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>Normally volatile Tesla $TSLA is the IBD Stock...</td>\n",
       "      <td>20</td>\n",
       "      <td>Jun</td>\n",
       "      <td>01</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>20Jun01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179490</th>\n",
       "      <td>2020-06-01 18:42:10+00:00</td>\n",
       "      <td>Normally volatile Tesla $TSLA is the IBD Stock...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>Normally volatile Tesla $TSLA is the IBD Stock...</td>\n",
       "      <td>20</td>\n",
       "      <td>Jun</td>\n",
       "      <td>01</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>20Jun01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179491</th>\n",
       "      <td>2020-06-01 18:41:35+00:00</td>\n",
       "      <td>$TSLA up $51.00 from next suggested buy entry ...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>$TSLA up $51.00 from next suggested buy entry ...</td>\n",
       "      <td>20</td>\n",
       "      <td>Jun</td>\n",
       "      <td>01</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>20Jun01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179492</th>\n",
       "      <td>2020-06-01 18:41:32+00:00</td>\n",
       "      <td>@Desert_Trader81 $TSLA on the move crossing HO...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>$TSLA on the move crossing HOD $885 !!!! 900 w...</td>\n",
       "      <td>20</td>\n",
       "      <td>Jun</td>\n",
       "      <td>01</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>20Jun01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179493</th>\n",
       "      <td>2020-06-01 18:38:37+00:00</td>\n",
       "      <td>They are going to squeeze $TSLA into the close...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>They are going to squeeze $TSLA into the close...</td>\n",
       "      <td>20</td>\n",
       "      <td>Jun</td>\n",
       "      <td>01</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>20Jun01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179494 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             date  \\\n",
       "0       2021-09-18 19:56:59+00:00   \n",
       "1       2021-09-18 04:16:52+00:00   \n",
       "2       2021-09-16 19:01:28+00:00   \n",
       "3       2021-09-16 15:23:53+00:00   \n",
       "4       2021-09-16 15:23:11+00:00   \n",
       "...                           ...   \n",
       "179489  2020-06-01 18:42:22+00:00   \n",
       "179490  2020-06-01 18:42:10+00:00   \n",
       "179491  2020-06-01 18:41:35+00:00   \n",
       "179492  2020-06-01 18:41:32+00:00   \n",
       "179493  2020-06-01 18:38:37+00:00   \n",
       "\n",
       "                                                  content  likeCount lang  \\\n",
       "0       @The_RockTrading Bullish on $TSLA this week &a...          1   en   \n",
       "1       $TSLA now is the same as $aapl in the 80s ! @e...          1   en   \n",
       "2       Added more $TSLA and $aapl to long, because I ...          0   en   \n",
       "3       @NeilRog49855230 @Gays4Tesla @TheMaverickWS Th...          3   en   \n",
       "4       There is plenty of information available on-li...          0   en   \n",
       "...                                                   ...        ...  ...   \n",
       "179489  Normally volatile Tesla $TSLA is the IBD Stock...          1   en   \n",
       "179490  Normally volatile Tesla $TSLA is the IBD Stock...          1   en   \n",
       "179491  $TSLA up $51.00 from next suggested buy entry ...          1   en   \n",
       "179492  @Desert_Trader81 $TSLA on the move crossing HO...          0   en   \n",
       "179493  They are going to squeeze $TSLA into the close...          0   en   \n",
       "\n",
       "                                          cleaned_content year month day  \\\n",
       "0                      Bullish on $TSLA this week & $Aapl   21   Sep  18   \n",
       "1             $TSLA now is the same as $aapl in the 80s !   21   Sep  18   \n",
       "2       Added more $TSLA and $aapl to long, because I ...   21   Sep  16   \n",
       "3       There is plenty of information available on-li...   21   Sep  16   \n",
       "4       There is plenty of information available on-li...   21   Sep  16   \n",
       "...                                                   ...  ...   ...  ..   \n",
       "179489  Normally volatile Tesla $TSLA is the IBD Stock...   20   Jun  01   \n",
       "179490  Normally volatile Tesla $TSLA is the IBD Stock...   20   Jun  01   \n",
       "179491  $TSLA up $51.00 from next suggested buy entry ...   20   Jun  01   \n",
       "179492  $TSLA on the move crossing HOD $885 !!!! 900 w...   20   Jun  01   \n",
       "179493  They are going to squeeze $TSLA into the close...   20   Jun  01   \n",
       "\n",
       "       ticker YearMonDay  \n",
       "0        TSLA    21Sep18  \n",
       "1        TSLA    21Sep18  \n",
       "2        TSLA    21Sep16  \n",
       "3        TSLA    21Sep16  \n",
       "4        TSLA    21Sep16  \n",
       "...       ...        ...  \n",
       "179489   TSLA    20Jun01  \n",
       "179490   TSLA    20Jun01  \n",
       "179491   TSLA    20Jun01  \n",
       "179492   TSLA    20Jun01  \n",
       "179493   TSLA    20Jun01  \n",
       "\n",
       "[179494 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df_dict['TSLA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Prepare the datasets to be used in the BERT model for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Clean the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text) #remove hashtags\n",
    "    text = re.sub(r'http\\S+', '', text)    #remove urls\n",
    "    text = re.sub(r'&amp;amp', '&', text)  #remove double amps\n",
    "    text = re.sub(r'\\&amp;', '&', text)    #remove single amps\n",
    "    text = re.sub(r'\\s+', ' ', text)       #reduce multiple spaces into a single space\n",
    "    text = re.sub(r'\\s+-\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tick, tick_df in reddit_df_dict.items():\n",
    "#     print(tick, tick_df)\n",
    "    tick_df['cleaned_body'] = tick_df['body'].apply(lambda x: text_preprocessing(x))\n",
    "    tick_df['LEN'] = tick_df.cleaned_body.str.len()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tick, tick_df in twitter_df_dict.items():\n",
    "    tick_df['cleaned_body'] = tick_df['cleaned_content'].apply(lambda x: text_preprocessing(x))\n",
    "    tick_df['LEN'] = tick_df.cleaned_body.str.len()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Handle records where text is greater than a length of 512\n",
    "- *BERT can only handle a max length of 512*\n",
    "- *For each comment > length of 512, break the comment into multiple sets of 512*\n",
    "- *After breaking, create a new record for each of the broken parts and append to the original dataframe. Since we take the average sentiment scores, appending new rows with the same dates will not affect the data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=512\n",
    "def ffn(xseries):\n",
    "    list_of_lists = []\n",
    "        \n",
    "    if xseries['LEN']>512:\n",
    "        parts = [xseries['cleaned_body'][i:i+n] for i in range(0, xseries['LEN'], n)]\n",
    "    \n",
    "        counter=0\n",
    "        for i in parts:\n",
    "            list_series = []\n",
    "            list_series.append(xseries['body'])\n",
    "            list_series.append(xseries['created_utc'])\n",
    "            list_series.append(xseries['id'])\n",
    "            list_series.append(xseries['top'])\n",
    "            list_series.append(xseries['year'])\n",
    "            list_series.append(xseries['month'])\n",
    "            list_series.append(xseries['day'])\n",
    "            list_series.append(xseries['ticker'])\n",
    "            list_series.append(xseries['YearMonDay'])\n",
    "            list_series.append(i)\n",
    "            counter+=1\n",
    "            list_series.append(counter)\n",
    "            \n",
    "            list_of_lists.append(list_series)\n",
    "    \n",
    "        return list_of_lists\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tick, tick_df in reddit_df_dict.items():\n",
    "    tick_df['mltp'] = tick_df.apply(ffn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tick, tick_df in twitter_df_dict.items():\n",
    "    tick_df['mltp'] = tick_df.apply(ffn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tick, tick_df in reddit_df_dict.items():\n",
    "    addition_list = tick_df.query('mltp != 0')['mltp']\n",
    "    \n",
    "    if len(addition_list)==0:\n",
    "        continue\n",
    "    \n",
    "    ind_additions = []\n",
    "\n",
    "    for list_of_sentences in addition_list:\n",
    "        for sentence_part in list_of_sentences:\n",
    "            ind_additions.append(sentence_part)\n",
    "    \n",
    "    \n",
    "    df_additions = pd.DataFrame(ind_additions, columns=['body','created_utc','id','top','year','month','day','ticker','YearMonDay','cleaned_body','LEN'])\n",
    "    \n",
    "    tick_df.drop('mltp', axis=1, inplace=True)\n",
    "    tick_df.drop(tick_df[tick_df.LEN > 512].index, inplace = True)\n",
    "    tick_df = tick_df.append(df_additions, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tick, tick_df in twitter_df_dict.items():\n",
    "    addition_list = tick_df.query('mltp != 0')['mltp']\n",
    "    \n",
    "    if len(addition_list)==0:\n",
    "        continue\n",
    "    \n",
    "    ind_additions = []\n",
    "\n",
    "    for list_of_sentences in addition_list:\n",
    "        for sentence_part in list_of_sentences:\n",
    "            ind_additions.append(sentence_part)\n",
    "    \n",
    "    \n",
    "    df_additions = pd.DataFrame(ind_additions, columns=['body','created_utc','id','top','year','month','day','ticker','YearMonDay','cleaned_body','LEN'])\n",
    "    \n",
    "    tick_df.drop('mltp', axis=1, inplace=True)\n",
    "    tick_df.drop(tick_df[tick_df.LEN > 512].index, inplace = True)\n",
    "    tick_df = tick_df.append(df_additions, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Make sure there is no data where comment length is greater than 512*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [body, created_utc, id, top, year, month, day, ticker, YearMonDay, cleaned_body, LEN]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [body, created_utc, id, top, year, month, day, ticker, YearMonDay, cleaned_body, LEN]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [body, created_utc, id, top, year, month, day, ticker, YearMonDay, cleaned_body, LEN]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [body, created_utc, id, top, year, month, day, ticker, YearMonDay, cleaned_body, LEN]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [body, created_utc, id, top, year, month, day, ticker, YearMonDay, cleaned_body, LEN]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [body, created_utc, id, top, year, month, day, ticker, YearMonDay, cleaned_body, LEN]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "for tick, tick_df in reddit_df_dict.items():\n",
    "    print(tick_df.query('LEN > 512'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [date, content, likeCount, lang, cleaned_content, year, month, day, ticker, YearMonDay, cleaned_body, LEN, mltp]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [date, content, likeCount, lang, cleaned_content, year, month, day, ticker, YearMonDay, cleaned_body, LEN, mltp]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [date, content, likeCount, lang, cleaned_content, year, month, day, ticker, YearMonDay, cleaned_body, LEN, mltp]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [date, content, likeCount, lang, cleaned_content, year, month, day, ticker, YearMonDay, cleaned_body, LEN, mltp]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [date, content, likeCount, lang, cleaned_content, year, month, day, ticker, YearMonDay, cleaned_body, LEN, mltp]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [date, content, likeCount, lang, cleaned_content, year, month, day, ticker, YearMonDay, cleaned_body, LEN, mltp]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "for tick, tick_df in twitter_df_dict.items():\n",
    "    print(tick_df.query('LEN > 512'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use finBERT model and get outputs for each comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create a huggingface pipeline with finBERT model\n",
    "- *set return_all_scores=True so that we receive scores for all 3 sentiments (positive, negative and neutral)*\n",
    "- *Ref: https://huggingface.co/transformers/main_classes/pipelines.html*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline('sentiment-analysis', model='ProsusAI/finbert', return_all_scores =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Get the sentiments for each day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Get the BERT output for reddit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tick, tick_df in reddit_df_dict.items():\n",
    "    BERT_yield = tick_df['cleaned_body'].apply(lambda x: classifier(x))\n",
    "    #---The output from finBERT is in the form of a list of list of dicts i.e. \"[[{key: value}]]\"\n",
    "    #---We convert it to a dataframe to make it easier for further steps\n",
    "    temp_list = []\n",
    "    for i in BERT_yield:\n",
    "        temp_dict={}\n",
    "        for j in range(0,3):\n",
    "            k = i[0][j][\"label\"]\n",
    "            v = i[0][j][\"score\"]\n",
    "            temp_dict[k]=v\n",
    "        temp_list.append(temp_dict)\n",
    "\n",
    "    BERT_output_df = pd.DataFrame(temp_list)\n",
    "    complete_output_df = pd.concat([tick_df, BERT_output_df], axis=1)\n",
    "    sentiment_df = complete_output_df.groupby(['ticker', 'YearMonDay'])[['positive', 'neutral', 'negative']].mean().reset_index()\n",
    "    reddit_df_dict[tick] = sentiment_df\n",
    "    sentiment_df.to_pickle(f\"C:\\\\Users\\\\Karthik\\\\Desktop\\\\Dissertation\\\\Final_dfs\\\\reddit_{tick}_finBERT.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Get the BERT output for twitter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tick, tick_df in twitter_df_dict.items():\n",
    "    BERT_yield = tick_df['cleaned_body'].apply(lambda x: classifier(x))\n",
    "    #---The output from finBERT is in the form of a list of list of dicts i.e. \"[[{key: value}]]\"\n",
    "    #---We convert it to a dataframe to make it easier for further steps\n",
    "    temp_list = []\n",
    "    for i in BERT_yield:\n",
    "        temp_dict={}\n",
    "        for j in range(0,3):\n",
    "            k = i[0][j][\"label\"]\n",
    "            v = i[0][j][\"score\"]\n",
    "            temp_dict[k]=v\n",
    "        temp_list.append(temp_dict)\n",
    "\n",
    "    BERT_output_df = pd.DataFrame(temp_list)\n",
    "    complete_output_df = pd.concat([tick_df, BERT_output_df], axis=1)\n",
    "    sentiment_df = complete_output_df.groupby(['ticker', 'YearMonDay'])[['positive', 'neutral', 'negative']].mean().reset_index()\n",
    "    twitter_df_dict[tick] = sentiment_df\n",
    "    sentiment_df.to_pickle(f\"C:\\\\Users\\\\Karthik\\\\Desktop\\\\Dissertation\\\\Final_dfs\\\\twitter_{tick}_finBERT.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Calculate combined predictions by combining both reddit and twitter sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL\n",
      "    ticker YearMonDay   neutral  positive  negative\n",
      "0     AAPL    20Aug01  0.666970  0.201389  0.131641\n",
      "1     AAPL    20Aug02  0.701786  0.187726  0.110488\n",
      "2     AAPL    20Aug03  0.697561  0.167927  0.134512\n",
      "3     AAPL    20Aug04  0.706635  0.177406  0.115958\n",
      "4     AAPL    20Aug05  0.635346  0.134119  0.230535\n",
      "..     ...        ...       ...       ...       ...\n",
      "499   AAPL    21Sep26  0.730306  0.135970  0.133725\n",
      "500   AAPL    21Sep27  0.709497  0.151975  0.138528\n",
      "501   AAPL    21Sep28  0.633344  0.160007  0.206648\n",
      "502   AAPL    21Sep29  0.687631  0.173307  0.139062\n",
      "503   AAPL    21Sep30  0.659920  0.169118  0.170962\n",
      "\n",
      "[504 rows x 5 columns]\n",
      "AMC\n",
      "    ticker YearMonDay   neutral  positive  negative\n",
      "0      AMC    20Aug01  0.743704  0.131487  0.124809\n",
      "1      AMC    20Aug02  0.680545  0.044179  0.275276\n",
      "2      AMC    20Aug03  0.539083  0.185692  0.275225\n",
      "3      AMC    20Aug04  0.594843  0.235051  0.170106\n",
      "4      AMC    20Aug05  0.635311  0.262705  0.101984\n",
      "..     ...        ...       ...       ...       ...\n",
      "488    AMC    21Sep26  0.805648  0.115804  0.078547\n",
      "489    AMC    21Sep27  0.792233  0.087659  0.120108\n",
      "490    AMC    21Sep28  0.790159  0.070514  0.139327\n",
      "491    AMC    21Sep29  0.790243  0.086273  0.123484\n",
      "492    AMC    21Sep30  0.780977  0.139895  0.079128\n",
      "\n",
      "[493 rows x 5 columns]\n",
      "DKNG\n",
      "    ticker YearMonDay   neutral  positive  negative\n",
      "0     DKNG    20Aug01  0.674635  0.169827  0.155538\n",
      "1     DKNG    20Aug02  0.703412  0.149141  0.147447\n",
      "2     DKNG    20Aug03  0.662906  0.192861  0.144233\n",
      "3     DKNG    20Aug04  0.617604  0.141499  0.240896\n",
      "4     DKNG    20Aug05  0.787257  0.123877  0.088866\n",
      "..     ...        ...       ...       ...       ...\n",
      "500   DKNG    21Sep26  0.606892  0.248563  0.144545\n",
      "501   DKNG    21Sep27  0.656234  0.198371  0.145394\n",
      "502   DKNG    21Sep28  0.628361  0.196809  0.174830\n",
      "503   DKNG    21Sep29  0.647138  0.184956  0.167907\n",
      "504   DKNG    21Sep30  0.586450  0.233751  0.179799\n",
      "\n",
      "[505 rows x 5 columns]\n",
      "TSLA\n",
      "    ticker YearMonDay   neutral  positive  negative\n",
      "0     TSLA    20Aug01  0.707172  0.169375  0.123452\n",
      "1     TSLA    20Aug02  0.736393  0.158933  0.104675\n",
      "2     TSLA    20Aug03  0.694593  0.136386  0.169021\n",
      "3     TSLA    20Aug04  0.646101  0.165671  0.188228\n",
      "4     TSLA    20Aug05  0.690832  0.149527  0.159641\n",
      "..     ...        ...       ...       ...       ...\n",
      "500   TSLA    21Sep26  0.713898  0.178083  0.108018\n",
      "501   TSLA    21Sep27  0.727954  0.159259  0.112786\n",
      "502   TSLA    21Sep28  0.673238  0.173674  0.153089\n",
      "503   TSLA    21Sep29  0.735544  0.148782  0.115673\n",
      "504   TSLA    21Sep30  0.759710  0.118137  0.122153\n",
      "\n",
      "[505 rows x 5 columns]\n",
      "AMD\n",
      "    ticker YearMonDay   neutral  positive  negative\n",
      "0      AMD    20Aug03  0.798593  0.182657  0.018750\n",
      "1      AMD    20Aug05  0.528485  0.399696  0.071819\n",
      "2      AMD    20Aug06  0.739396  0.125557  0.135047\n",
      "3      AMD    20Aug08  0.817001  0.164177  0.018823\n",
      "4      AMD    20Aug10  0.706631  0.105069  0.188300\n",
      "..     ...        ...       ...       ...       ...\n",
      "384    AMD    21Sep26  0.684413  0.219662  0.095924\n",
      "385    AMD    21Sep27  0.621320  0.304300  0.074380\n",
      "386    AMD    21Sep28  0.684309  0.160136  0.155555\n",
      "387    AMD    21Sep29  0.685541  0.171201  0.143258\n",
      "388    AMD    21Sep30  0.656336  0.261367  0.082297\n",
      "\n",
      "[389 rows x 5 columns]\n",
      "BABA\n",
      "    ticker YearMonDay   neutral  positive  negative\n",
      "0     BABA    20Aug01  0.556968  0.217219  0.225813\n",
      "1     BABA    20Aug02  0.657105  0.215102  0.127792\n",
      "2     BABA    20Aug03  0.683406  0.177035  0.139560\n",
      "3     BABA    20Aug04  0.620285  0.233368  0.146347\n",
      "4     BABA    20Aug05  0.744712  0.189596  0.065691\n",
      "..     ...        ...       ...       ...       ...\n",
      "501   BABA    21Sep26  0.670115  0.176349  0.153536\n",
      "502   BABA    21Sep27  0.662315  0.219888  0.117797\n",
      "503   BABA    21Sep28  0.617888  0.233867  0.148244\n",
      "504   BABA    21Sep29  0.678987  0.140842  0.180172\n",
      "505   BABA    21Sep30  0.699849  0.155136  0.145014\n",
      "\n",
      "[506 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "for ticker in tick_list:\n",
    "    print(ticker)\n",
    "    reddit_sentis = reddit_df_dict[ticker][['ticker', 'YearMonDay', 'neutral', 'positive', 'negative']]\n",
    "    twitter_sentis = twitter_df_dict[ticker][['ticker', 'YearMonDay', 'neutral', 'positive', 'negative']]\n",
    "    \n",
    "#     print(reddit_sentis,twitter_sentis,pd.concat([reddit_sentis,twitter_sentis], axis=0,ignore_index=True))\n",
    "    combined_sentis = pd.concat([reddit_sentis,twitter_sentis], axis=0,ignore_index=True)\n",
    "    combined_sentis_grouped = combined_sentis.groupby(['ticker', 'YearMonDay'])[['neutral', 'positive', 'negative']].mean().reset_index()\n",
    "    combined_sentis_grouped.to_pickle(f\"C:\\\\Users\\\\Karthik\\\\Desktop\\\\Dissertation\\\\Final_dfs\\\\combined_{ticker}_finBERT.pkl\")\n",
    "    print(combined_sentis_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ref\n",
    "https://huggingface.co/transformers/main_classes/pipelines.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
