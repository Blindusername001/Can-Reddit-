{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3A: BERT Model 1 - BERT sentiment analysis using US airline sentiment dataset (equalized sentiment count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Required imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Neural Network libraries\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup            #huggingface transformers library\n",
    "import tensorflow as tf                                                                                                  #tensorflow library         \n",
    "import torch                                                                                                             #pytorch library\n",
    "import torch.nn.functional as Func\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, RandomSampler, SequentialSampler                        #for loading data into our model\n",
    "\n",
    "#---Data processing\n",
    "from sklearn.model_selection import train_test_split                                                                     #for splitting data into training, testing and validation\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd                                                                                                      #for using data in the form of dataframes\n",
    "import numpy as np\n",
    "import re                                                                                                                #for data manipulation when cleaning datasets\n",
    "import os\n",
    "\n",
    "#---Visualization libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4f4ac6ea054806a5b98f2fe2d17ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "widgets.IntSlider()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import training data to train the BERT model\n",
    "- *This dataset is from kaggle - https://www.kaggle.com/crowdflower/twitter-airline-sentiment?select=Tweets.csv*\n",
    "- *We only pick the required columns from the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>570306133677760513</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570301130888122368</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570301083672813571</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570301031407624196</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570300817074462722</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   airline_sentiment  \\\n",
       "tweet_id                               \n",
       "570306133677760513           neutral   \n",
       "570301130888122368          positive   \n",
       "570301083672813571           neutral   \n",
       "570301031407624196          negative   \n",
       "570300817074462722          negative   \n",
       "\n",
       "                                                                 text  \n",
       "tweet_id                                                               \n",
       "570306133677760513                @VirginAmerica What @dhepburn said.  \n",
       "570301130888122368  @VirginAmerica plus you've added commercials t...  \n",
       "570301083672813571  @VirginAmerica I didn't today... Must mean I n...  \n",
       "570301031407624196  @VirginAmerica it's really aggressive to blast...  \n",
       "570300817074462722  @VirginAmerica and it's a really big bad thing...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = pd.read_csv(r\"C:\\Users\\Karthik\\Desktop\\Dissertation\\BERT\\Training dataset\\Tweets.csv\")\n",
    "training_data = training_data[['tweet_id', 'airline_sentiment', 'text']]\n",
    "training_data.set_index('tweet_id', inplace=True)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Import reddit and twitter data for which we have to populate the sentiments\n",
    "- *we will use them on the model after the model is trained with the training dataset from kaggle*\n",
    "- *we save data in two dictionaries; one for reddit and one for twitter to make it more organized*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick_list =  ['AMC', 'DKNG', 'TSLA', 'AMD', 'BABA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df_dict = {tick: pd.read_pickle(f\"C:\\\\Users\\\\Karthik\\\\Desktop\\\\Dissertation\\\\Reddit\\\\consolidated_pickle_files\\\\reddit_{tick}_df_for_BERT.pkl\") for tick in tick_list}\n",
    "# reddit_df_dict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>top</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>ticker</th>\n",
       "      <th>YearMonDay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Non-troll post. I started 2 weeks ago and have...</td>\n",
       "      <td>1609500199</td>\n",
       "      <td>ghp72zs</td>\n",
       "      <td>top</td>\n",
       "      <td>21</td>\n",
       "      <td>Jan</td>\n",
       "      <td>01</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>21Jan01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>we eat cornbread on new years day to ensure a ...</td>\n",
       "      <td>1609524808</td>\n",
       "      <td>ghqo6qb</td>\n",
       "      <td>top</td>\n",
       "      <td>21</td>\n",
       "      <td>Jan</td>\n",
       "      <td>01</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>21Jan01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TSLA 850 EOD</td>\n",
       "      <td>1612203941</td>\n",
       "      <td>glmp0gv</td>\n",
       "      <td>top</td>\n",
       "      <td>21</td>\n",
       "      <td>Feb</td>\n",
       "      <td>01</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>21Feb01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>joe weisenthal is the ultimate chad( who else ...</td>\n",
       "      <td>1612207510</td>\n",
       "      <td>glmyg66</td>\n",
       "      <td>top</td>\n",
       "      <td>21</td>\n",
       "      <td>Feb</td>\n",
       "      <td>01</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>21Feb01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TSLA ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€</td>\n",
       "      <td>1612211972</td>\n",
       "      <td>gln9v98</td>\n",
       "      <td>top</td>\n",
       "      <td>21</td>\n",
       "      <td>Feb</td>\n",
       "      <td>01</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>21Feb01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15125</th>\n",
       "      <td>TSLA Drill Team 6 reporting for duty</td>\n",
       "      <td>1609359381</td>\n",
       "      <td>ghj65jr</td>\n",
       "      <td>top</td>\n",
       "      <td>20</td>\n",
       "      <td>Dec</td>\n",
       "      <td>30</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>20Dec30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15126</th>\n",
       "      <td>TSLAâ€™s still having them TSLA days I see. Join...</td>\n",
       "      <td>1609359393</td>\n",
       "      <td>ghj66f1</td>\n",
       "      <td>top</td>\n",
       "      <td>20</td>\n",
       "      <td>Dec</td>\n",
       "      <td>30</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>20Dec30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15127</th>\n",
       "      <td>TSLA ðŸŽ° EOY coming</td>\n",
       "      <td>1609360555</td>\n",
       "      <td>ghj8jam</td>\n",
       "      <td>top</td>\n",
       "      <td>20</td>\n",
       "      <td>Dec</td>\n",
       "      <td>30</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>20Dec30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15128</th>\n",
       "      <td>Thank fuck I didn't dump my TSLA calls yesterday</td>\n",
       "      <td>1609361591</td>\n",
       "      <td>ghjamup</td>\n",
       "      <td>top</td>\n",
       "      <td>20</td>\n",
       "      <td>Dec</td>\n",
       "      <td>30</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>20Dec30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15129</th>\n",
       "      <td>What date is TSLA announcing delivery numbers?</td>\n",
       "      <td>1609361685</td>\n",
       "      <td>ghjatox</td>\n",
       "      <td>top</td>\n",
       "      <td>20</td>\n",
       "      <td>Dec</td>\n",
       "      <td>30</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>20Dec30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15130 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    body  created_utc  \\\n",
       "0      Non-troll post. I started 2 weeks ago and have...   1609500199   \n",
       "1      we eat cornbread on new years day to ensure a ...   1609524808   \n",
       "2                                           TSLA 850 EOD   1612203941   \n",
       "3      joe weisenthal is the ultimate chad( who else ...   1612207510   \n",
       "4                                             TSLA ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€   1612211972   \n",
       "...                                                  ...          ...   \n",
       "15125               TSLA Drill Team 6 reporting for duty   1609359381   \n",
       "15126  TSLAâ€™s still having them TSLA days I see. Join...   1609359393   \n",
       "15127                                  TSLA ðŸŽ° EOY coming   1609360555   \n",
       "15128   Thank fuck I didn't dump my TSLA calls yesterday   1609361591   \n",
       "15129     What date is TSLA announcing delivery numbers?   1609361685   \n",
       "\n",
       "            id  top year month day ticker YearMonDay  \n",
       "0      ghp72zs  top   21   Jan  01   TSLA    21Jan01  \n",
       "1      ghqo6qb  top   21   Jan  01   TSLA    21Jan01  \n",
       "2      glmp0gv  top   21   Feb  01   TSLA    21Feb01  \n",
       "3      glmyg66  top   21   Feb  01   TSLA    21Feb01  \n",
       "4      gln9v98  top   21   Feb  01   TSLA    21Feb01  \n",
       "...        ...  ...  ...   ...  ..    ...        ...  \n",
       "15125  ghj65jr  top   20   Dec  30   TSLA    20Dec30  \n",
       "15126  ghj66f1  top   20   Dec  30   TSLA    20Dec30  \n",
       "15127  ghj8jam  top   20   Dec  30   TSLA    20Dec30  \n",
       "15128  ghjamup  top   20   Dec  30   TSLA    20Dec30  \n",
       "15129  ghjatox  top   20   Dec  30   TSLA    20Dec30  \n",
       "\n",
       "[15130 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df_dict['TSLA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df_dict = {tick: pd.read_pickle(f\"C:\\\\Users\\\\Karthik\\\\Desktop\\\\Dissertation\\\\Twitter\\\\consolidated_pickle_files\\\\twitter_{tick}_df_for_BERT.pkl\") for tick in tick_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>lang</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>ticker</th>\n",
       "      <th>YearMonDay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-09-18 19:56:59+00:00</td>\n",
       "      <td>@The_RockTrading Bullish on $TSLA this week &amp;a...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>Bullish on $TSLA this week &amp; $Aapl</td>\n",
       "      <td>21</td>\n",
       "      <td>Sep</td>\n",
       "      <td>18</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>21Sep18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-09-18 04:16:52+00:00</td>\n",
       "      <td>$TSLA now is the same as $aapl in the 80s ! @e...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>$TSLA now is the same as $aapl in the 80s !</td>\n",
       "      <td>21</td>\n",
       "      <td>Sep</td>\n",
       "      <td>18</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>21Sep18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-09-16 19:01:28+00:00</td>\n",
       "      <td>Added more $TSLA and $aapl to long, because I ...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>Added more $TSLA and $aapl to long, because I ...</td>\n",
       "      <td>21</td>\n",
       "      <td>Sep</td>\n",
       "      <td>16</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>21Sep16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-09-16 15:23:53+00:00</td>\n",
       "      <td>@NeilRog49855230 @Gays4Tesla @TheMaverickWS Th...</td>\n",
       "      <td>3</td>\n",
       "      <td>en</td>\n",
       "      <td>There is plenty of information available on-li...</td>\n",
       "      <td>21</td>\n",
       "      <td>Sep</td>\n",
       "      <td>16</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>21Sep16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-09-16 15:23:11+00:00</td>\n",
       "      <td>There is plenty of information available on-li...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>There is plenty of information available on-li...</td>\n",
       "      <td>21</td>\n",
       "      <td>Sep</td>\n",
       "      <td>16</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>21Sep16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179489</th>\n",
       "      <td>2020-06-01 18:42:22+00:00</td>\n",
       "      <td>Normally volatile Tesla $TSLA is the IBD Stock...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>Normally volatile Tesla $TSLA is the IBD Stock...</td>\n",
       "      <td>20</td>\n",
       "      <td>Jun</td>\n",
       "      <td>01</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>20Jun01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179490</th>\n",
       "      <td>2020-06-01 18:42:10+00:00</td>\n",
       "      <td>Normally volatile Tesla $TSLA is the IBD Stock...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>Normally volatile Tesla $TSLA is the IBD Stock...</td>\n",
       "      <td>20</td>\n",
       "      <td>Jun</td>\n",
       "      <td>01</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>20Jun01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179491</th>\n",
       "      <td>2020-06-01 18:41:35+00:00</td>\n",
       "      <td>$TSLA up $51.00 from next suggested buy entry ...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>$TSLA up $51.00 from next suggested buy entry ...</td>\n",
       "      <td>20</td>\n",
       "      <td>Jun</td>\n",
       "      <td>01</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>20Jun01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179492</th>\n",
       "      <td>2020-06-01 18:41:32+00:00</td>\n",
       "      <td>@Desert_Trader81 $TSLA on the move crossing HO...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>$TSLA on the move crossing HOD $885 !!!! 900 w...</td>\n",
       "      <td>20</td>\n",
       "      <td>Jun</td>\n",
       "      <td>01</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>20Jun01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179493</th>\n",
       "      <td>2020-06-01 18:38:37+00:00</td>\n",
       "      <td>They are going to squeeze $TSLA into the close...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>They are going to squeeze $TSLA into the close...</td>\n",
       "      <td>20</td>\n",
       "      <td>Jun</td>\n",
       "      <td>01</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>20Jun01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179494 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             date  \\\n",
       "0       2021-09-18 19:56:59+00:00   \n",
       "1       2021-09-18 04:16:52+00:00   \n",
       "2       2021-09-16 19:01:28+00:00   \n",
       "3       2021-09-16 15:23:53+00:00   \n",
       "4       2021-09-16 15:23:11+00:00   \n",
       "...                           ...   \n",
       "179489  2020-06-01 18:42:22+00:00   \n",
       "179490  2020-06-01 18:42:10+00:00   \n",
       "179491  2020-06-01 18:41:35+00:00   \n",
       "179492  2020-06-01 18:41:32+00:00   \n",
       "179493  2020-06-01 18:38:37+00:00   \n",
       "\n",
       "                                                  content  likeCount lang  \\\n",
       "0       @The_RockTrading Bullish on $TSLA this week &a...          1   en   \n",
       "1       $TSLA now is the same as $aapl in the 80s ! @e...          1   en   \n",
       "2       Added more $TSLA and $aapl to long, because I ...          0   en   \n",
       "3       @NeilRog49855230 @Gays4Tesla @TheMaverickWS Th...          3   en   \n",
       "4       There is plenty of information available on-li...          0   en   \n",
       "...                                                   ...        ...  ...   \n",
       "179489  Normally volatile Tesla $TSLA is the IBD Stock...          1   en   \n",
       "179490  Normally volatile Tesla $TSLA is the IBD Stock...          1   en   \n",
       "179491  $TSLA up $51.00 from next suggested buy entry ...          1   en   \n",
       "179492  @Desert_Trader81 $TSLA on the move crossing HO...          0   en   \n",
       "179493  They are going to squeeze $TSLA into the close...          0   en   \n",
       "\n",
       "                                          cleaned_content year month day  \\\n",
       "0                      Bullish on $TSLA this week & $Aapl   21   Sep  18   \n",
       "1             $TSLA now is the same as $aapl in the 80s !   21   Sep  18   \n",
       "2       Added more $TSLA and $aapl to long, because I ...   21   Sep  16   \n",
       "3       There is plenty of information available on-li...   21   Sep  16   \n",
       "4       There is plenty of information available on-li...   21   Sep  16   \n",
       "...                                                   ...  ...   ...  ..   \n",
       "179489  Normally volatile Tesla $TSLA is the IBD Stock...   20   Jun  01   \n",
       "179490  Normally volatile Tesla $TSLA is the IBD Stock...   20   Jun  01   \n",
       "179491  $TSLA up $51.00 from next suggested buy entry ...   20   Jun  01   \n",
       "179492  $TSLA on the move crossing HOD $885 !!!! 900 w...   20   Jun  01   \n",
       "179493  They are going to squeeze $TSLA into the close...   20   Jun  01   \n",
       "\n",
       "       ticker YearMonDay  \n",
       "0        TSLA    21Sep18  \n",
       "1        TSLA    21Sep18  \n",
       "2        TSLA    21Sep16  \n",
       "3        TSLA    21Sep16  \n",
       "4        TSLA    21Sep16  \n",
       "...       ...        ...  \n",
       "179489   TSLA    20Jun01  \n",
       "179490   TSLA    20Jun01  \n",
       "179491   TSLA    20Jun01  \n",
       "179492   TSLA    20Jun01  \n",
       "179493   TSLA    20Jun01  \n",
       "\n",
       "[179494 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df_dict['TSLA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Get equal label counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    9178\n",
       "neutral     3099\n",
       "positive    2363\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.airline_sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.concat([training_data.query('airline_sentiment==\"negative\"').sample(n=2363),training_data.query('airline_sentiment==\"neutral\"').sample(n=2363),training_data.query('airline_sentiment==\"positive\"')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     2363\n",
       "negative    2363\n",
       "positive    2363\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.airline_sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 One-Hot encode the labels\n",
    "*There are 3 lables so we can encode the lables with 0,1 and 2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>569893064342437888</th>\n",
       "      <td>negative</td>\n",
       "      <td>@AmericanAir Thanks for the response.Tough nig...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569740152966238208</th>\n",
       "      <td>negative</td>\n",
       "      <td>@AmericanAir I've been on hold ANOTHER hour an...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569875203473297408</th>\n",
       "      <td>negative</td>\n",
       "      <td>@united UA1130 Flight was a nightmare!! From p...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570306529947193344</th>\n",
       "      <td>negative</td>\n",
       "      <td>@AmericanAir I slept in the miami airport due ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569311060903268352</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica a lot of \"apologies\" being thro...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   airline_sentiment  \\\n",
       "tweet_id                               \n",
       "569893064342437888          negative   \n",
       "569740152966238208          negative   \n",
       "569875203473297408          negative   \n",
       "570306529947193344          negative   \n",
       "569311060903268352          negative   \n",
       "\n",
       "                                                                 text  label  \n",
       "tweet_id                                                                      \n",
       "569893064342437888  @AmericanAir Thanks for the response.Tough nig...      2  \n",
       "569740152966238208  @AmericanAir I've been on hold ANOTHER hour an...      2  \n",
       "569875203473297408  @united UA1130 Flight was a nightmare!! From p...      2  \n",
       "570306529947193344  @AmericanAir I slept in the miami airport due ...      2  \n",
       "569311060903268352  @VirginAmerica a lot of \"apologies\" being thro...      2  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['label'] = training_data.airline_sentiment.replace({'neutral':0, 'positive':1, 'negative':2})\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Define data cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text) #remove hashtags\n",
    "    text = re.sub(r'http\\S+', '', text)    #remove urls\n",
    "    text = re.sub(r'&amp;amp', '&', text)  #remove double amps\n",
    "    text = re.sub(r'\\&amp;', '&', text)    #remove single amps\n",
    "    text = re.sub(r'\\s+', ' ', text)       #reduce multiple spaces into a single space\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Make a copy of the original raw text for future reference if required\n",
    "training_data['original_text'] = training_data['text']\n",
    "#---Use the above function to clean the tweet data\n",
    "training_data['text'] = training_data.text.apply(lambda x: text_preprocessing(x))\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Split the cleaned training data into train and validation datasets\n",
    "- *we do not need a test dataset. This is because we are using the training dataset only to train the BERT model*\n",
    "- *since we are going to use the BERT model to classify a different corpus (reddit and twitter data), splitting the data into training and validation datasets serves our purpose*\n",
    "- *this will also help in having a bigger training dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>original_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>label</th>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">negative</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>train</th>\n",
       "      <td>1769</td>\n",
       "      <td>1769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>594</td>\n",
       "      <td>594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">neutral</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>train</th>\n",
       "      <td>1766</td>\n",
       "      <td>1766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>597</td>\n",
       "      <td>597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">positive</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>train</th>\n",
       "      <td>1761</td>\n",
       "      <td>1761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>602</td>\n",
       "      <td>602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 text  original_text\n",
       "airline_sentiment label dataset                     \n",
       "negative          2     train    1769           1769\n",
       "                        val       594            594\n",
       "neutral           0     train    1766           1766\n",
       "                        val       597            597\n",
       "positive          1     train    1761           1761\n",
       "                        val       602            602"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#---we will use scikit-learn's train_test_split to classify records into train, test or validation dataset\n",
    "x_train, x_val, y_train, y_val =  train_test_split(training_data.index.values,          #x values/ input values \n",
    "                                                   training_data.label.values,          #y values/ output values/ lables\n",
    "                                                   test_size=0.25,                      #percentage of data to be used for test dataset\n",
    "                                                   random_state=17,                     #shuffles the data prior to splitting\n",
    "                                                   stratify=training_data.label.values  #stratification helps when the distribution of data is uneven like in our case where,\n",
    "                                                                                        #we have a lot of negative compared to neutral and positive labels                                                   \n",
    ")\n",
    "\n",
    "#---add a new column to training data to classify records into train and val datasets\n",
    "training_data['dataset'] = 'NA'\n",
    "training_data.loc[x_train, 'dataset'] = 'train'\n",
    "training_data.loc[x_val, 'dataset'] = 'val'\n",
    "\n",
    "training_data.groupby(['airline_sentiment', 'label', 'dataset']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Tokenize and encode data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.1 Find the max tweet length in the training data\n",
    "- *This is required to be input into the tokenizer*\n",
    "- *BERT expects all its inputs to be of the same length so shorter sentences will be padded to maintain the length*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = training_data['text'].str.len().max()\n",
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2 Initialize huggingface tokenizer and tokenize and encode train and val datasets\n",
    "\n",
    "- *In this step we will be tokenizing and encoding the input side of our datasets (i.e) the tweet comments*\n",
    "- *We will be using batch_encode_plus() method in the tokenizer as our input to the tokenizer will be an array of the tweet column in training_data [training_data[training_data.dataset=='train'].text.values]*\n",
    "- Ref:*https://huggingface.co/transformers/internal/tokenization_utils.html#pretrainedtokenizerbase*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Initialize bert-base-uncased tokenizer and select the option to convert all text to lowercase\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2184: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#---Tokenize and encode train dataset\n",
    "train_enc = tokenizer.batch_encode_plus(training_data[training_data.dataset=='train'].text.values, \n",
    "                                           add_special_tokens=True, \n",
    "                                           return_attention_mask=True, \n",
    "                                           pad_to_max_length=True, \n",
    "                                           max_length=MAX_LEN, \n",
    "                                           return_tensors='pt'\n",
    "                                           )\n",
    "\n",
    "#---Tokenize and encode val dataset\n",
    "val_enc = tokenizer.batch_encode_plus(training_data[training_data.dataset=='val'].text.values, \n",
    "                                         add_special_tokens=True, \n",
    "                                         return_attention_mask=True, \n",
    "                                         pad_to_max_length=True, \n",
    "                                         max_length=MAX_LEN, \n",
    "                                         return_tensors='pt'\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Create tensor datasets from the encoded datasets from previous step\n",
    "- *We must split the input_ids(encoded tokens) and attention masks(says which values a model should work on since we padded tweets shorter than 172) from the previous step*\n",
    "- *we must create a tensor dataset with both input(encoded tweets from last step) and also outputs (the one-hot encoded lables)*\n",
    "- *we create a tensor with the outputs so that the TensorDataset method can combine both inputs and outputs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5296, 1793)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#---Get the input ids and attention masks from the tokenizer outputs\n",
    "train_input_ids = train_enc['input_ids']\n",
    "train_attn_masks = train_enc['attention_mask']\n",
    "val_input_ids = val_enc['input_ids']\n",
    "val_attn_masks = val_enc['attention_mask']\n",
    "\n",
    "#---Create tensors with the output data\n",
    "train_labels = torch.tensor(training_data[training_data.dataset=='train'].label.values)\n",
    "val_labels = torch.tensor(training_data[training_data.dataset=='val'].label.values)\n",
    "\n",
    "#---Create the required tensor datasets which will be used in the dataloader\n",
    "train_tensor = TensorDataset(train_input_ids, train_attn_masks, train_labels)\n",
    "val_tensor = TensorDataset(val_input_ids, val_attn_masks, val_labels)\n",
    "\n",
    "len(train_tensor), len(val_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Create dataloaders\n",
    "- *We create two dataloaders - one for training data and one for validation data*\n",
    "- *We use random sampling*\n",
    "- *We use a  batch size of 32 in consideration of the low computing power at hand. It is also known that smaller batch sizes converge quicker - this seems like a good choice considering our limited training data*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# We Need two different dataloder\n",
    "train_dloader = DataLoader(train_tensor, sampler=RandomSampler(train_tensor), batch_size=batch_size)\n",
    "val_dloader = DataLoader(val_tensor, sampler=RandomSampler(val_tensor), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BERT model and performance definitions\n",
    "- *Define BERT model, optimizer, scheduler and performance metrics*\n",
    "- *Ignore the warning because we are going to train the model before using it to classify reddit and twitter data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define the BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(training_data.label.unique()),    #Used for better code reproducability instead of initializing a constant value\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Move the model to GPU for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device) \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Define evaluation function with performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/transformers/main_classes/output.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(validation_dataloader):\n",
    "    #---set the model in evaluation mode to disable dropout layers, batch normalization, etc\n",
    "    model.eval()\n",
    "    \n",
    "    #---Initialize some variables to calculate model performance post evaluation\n",
    "    total_loss = 0                          #counter to keep a sum of loss values across all batches\n",
    "    predicted_class, true_class = [], []    #counters to keep track of all the predicted and true classes across all batches\n",
    "    \n",
    "    #---Use model to predict with validation dataset\n",
    "    for batch in tqdm(validation_dataloader):\n",
    "        \n",
    "        #---get the input_ids, attention_masks and labels from each batch int a dictionary to pass as input to the model\n",
    "        temp_batch = tuple(b.to(device) for b in batch)                                                         #move the batch elements to GPU for faster performance\n",
    "        input_dict = {'input_ids': temp_batch[0], 'attention_mask': temp_batch[1], 'labels': temp_batch[2]}\n",
    "        \n",
    "        #---enable no_grad option in pytorch as there will be no back-propogation during evaluation\n",
    "        with torch.no_grad():\n",
    "            #---Predict outputs using the model\n",
    "            model_output = model(**input_dict)\n",
    "\n",
    "        #---get loss for current batch and add to the total_loss counter\n",
    "        loss = model_output[0]             #Example output in every iteration: tensor(0.2658, device='cuda:0')\n",
    "        total_loss += loss.item()\n",
    "        #print('model op 0:', loss)\n",
    "        \n",
    "        #---get the predicted outputs (logits) for current batch and append to predicted_class list\n",
    "        logits = model_output[1]                          #List of lists [[0.1 0.2 0.3][0.1 0.2 0.3]...32 lists inside a list]\n",
    "        #move the logits to cpu and convert to numpy since we will compute the simple math error calculations in cpu and not GPU\n",
    "        logits = logits.detach().cpu().numpy()            #array of list of lists array([[0.1 0.2 0.3][0.1 0.2 0.3]...32 lists inside a list])\n",
    "        predicted_class.append(logits)                    #list containing array of list of lists [array, array, ...]\n",
    "        #print('model op 1:', logits)\n",
    "        #print('pred_class:', predicted_class)\n",
    "        \n",
    "        #---get the original true labels and append to true_class list\n",
    "        true_labels = input_dict['labels'].cpu().numpy()   #array of list of labels [0,1,2,2,1,0,...32 labels]\n",
    "        true_class.append(true_labels)                     #list of array of list of labels [array([32 labels]), array([32 labels]),...]\n",
    "        #print('true_class:', true_class)\n",
    "    \n",
    "    #---Compute average loss\n",
    "    average_loss = total_loss/len(validation_dataloader) \n",
    "    \n",
    "    #---Compute prediction accuracy\n",
    "    predictions = np.concatenate(predicted_class, axis=0)  #list of lists [[0.1 0.2 0.3][0.1 0.2 0.3]....]\n",
    "    true_vals = np.concatenate(true_class, axis=0)         #List of lists\n",
    "    correct_predictions = np.sum(np.argmax(predictions, axis=1).flatten() == true_vals)\n",
    "    prediction_accuracy = correct_predictions / len(training_data[training_data.dataset=='val'])\n",
    "    #print('predicaitons', predictions)\n",
    "    #print('true_vals', true_vals)\n",
    "    \n",
    "    #---Compute f1 score\n",
    "    flattened_predictions = np.argmax(predictions, axis=1).flatten()    #List of values in predictions\n",
    "    flattened_labels = true_vals.flatten()                              #List of values in true_vals\n",
    "    f1score = f1_score(flattened_labels, flattened_predictions, average='weighted')\n",
    "    #print('preds_flat:', preds_flat)\n",
    "    #print('labels_flat:', labels_flat)\n",
    "    \n",
    "    return average_loss, predictions, true_vals, prediction_accuracy, f1score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Clear GPU cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Allocations           |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Trial and error to find the best parameters\n",
    "- *The original paper for BERT worked on batch sizes of (16 and 32) and learning rates of (5e-5, 4e-5, 3e-5, 2e-5, 1e-5)*\n",
    "- REF: *https://arxiv.org/abs/1810.04805*\n",
    "- *We prefer AdamW optimizer function since it is the most widely used general purpose optimizer and Adam was the optimization function used in the original paper on BERT*\n",
    "- *The scheduler will help bring the learning rate down if it senses overfitting*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Learning Rate: 4e-5, Batch size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "lr = 4e-5\n",
    "optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dloader)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9801c001727847099f4261af679aa299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 1'), FloatProgress(value=0.0, max=166.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fda7bad5ea4623ab3c18ac0fbd650f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=57.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ì²EÌ²pÌ²oÌ²cÌ²hÌ² Ì²1\n",
      "Training loss: 0.6218656095036541\n",
      "Validation loss: 0.5115847117022464\n",
      "validation accuracy: 0.795872838817624\n",
      "f1 score: 0.7968358762379277\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 2'), FloatProgress(value=0.0, max=166.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4ba7a5b33840aca4b919cefe3af8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=57.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ì²EÌ²pÌ²oÌ²cÌ²hÌ² Ì²2\n",
      "Training loss: 0.34559505634817733\n",
      "Validation loss: 0.5546318757298746\n",
      "validation accuracy: 0.8014500836586727\n",
      "f1 score: 0.7993793464665216\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 3'), FloatProgress(value=0.0, max=166.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3470e3f1e2467aa403c23604be2c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=57.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ì²EÌ²pÌ²oÌ²cÌ²hÌ² Ì²3\n",
      "Training loss: 0.19899453961077224\n",
      "Validation loss: 0.5979253647505844\n",
      "validation accuracy: 0.8131622978248745\n",
      "f1 score: 0.8131363182657215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_loss_dict = {}\n",
    "validation_loss_dict = {}\n",
    "\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    #---set the model to training mode\n",
    "    model.train()         \n",
    "    \n",
    "    #counter to keep a sum of loss values across all epochs\n",
    "    total_loss = 0\n",
    "\n",
    "    # Setting up the Progress bar to Moniter the progress of training\n",
    "    pbar = tqdm(train_dloader, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in pbar:\n",
    "        \n",
    "        #---set zero_grad so that gradient values are not accumulated across batches\n",
    "        model.zero_grad()\n",
    "        \n",
    "        #---get the input_ids, attention_masks and labels from each batch int a dictionary to pass as input to the model\n",
    "        temp_batch = tuple(b.to(device) for b in batch)                 #move the batch elements to GPU for faster performance\n",
    "         \n",
    "        input_dict = {'input_ids': temp_batch[0], 'attention_mask': temp_batch[1], 'labels': temp_batch[2]}       \n",
    "\n",
    "        #---Predict outputs using the model\n",
    "        model_output = model(**input_dict)\n",
    "\n",
    "        #---get loss for current batch and add to the total_loss counter\n",
    "        loss = model_output[0]\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        #---compute and clip gradients\n",
    "        loss.backward()                                           #compute gradients via backpropogation\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   #clip gradients to prevent gradient explosion issue\n",
    "\n",
    "        #---Use optimizer and scheduler to tune the model during epochs\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        pbar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "    #---save the model to disk        \n",
    "    #torch.save(model.state_dict(), r'C:\\Users\\Karthik\\Desktop\\Dissertation\\BERT\\Model\\BERT_trained.model')\n",
    "        \n",
    "    #---calculate metrics \n",
    "    loss_train_avg = total_loss/len(train_dloader)            \n",
    "    val_loss, predictions, true_vals, val_accuracy, f1score = model_eval(val_dloader)\n",
    "\n",
    "    training_loss_dict[epoch] = loss_train_avg\n",
    "    validation_loss_dict[epoch] = val_loss\n",
    "    \n",
    "    tqdm.write(\"\\u0332\".join(f'\\nEpoch {epoch}'))\n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'validation accuracy: {val_accuracy}')\n",
    "    tqdm.write(f'f1 score: {f1score}')\n",
    "    \n",
    "torch.save(model, f\"C:\\\\Users\\\\Karthik\\\\Desktop\\\\Dissertation\\\\BERT\\\\Models\\\\BERT_TwitterUSAirline_Batch_{batch_size}_LR_{lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Learning Rate: 3e-5, Batch size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "lr = 3e-5\n",
    "optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dloader)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f322171961442db37d08bf42bc5445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 1'), FloatProgress(value=0.0, max=166.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be389e6fb6c64fc085a7424e66091354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=57.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ì²EÌ²pÌ²oÌ²cÌ²hÌ² Ì²1\n",
      "Training loss: 0.2014769176582256\n",
      "Validation loss: 0.708915053000837\n",
      "validation accuracy: 0.8008923591745678\n",
      "f1 score: 0.7994762140591439\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 2'), FloatProgress(value=0.0, max=166.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f772e841d0b64284a6ae3cdad8fa20a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=57.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ì²EÌ²pÌ²oÌ²cÌ²hÌ² Ì²2\n",
      "Training loss: 0.10953409520695996\n",
      "Validation loss: 0.7897362233253947\n",
      "validation accuracy: 0.8053541550474066\n",
      "f1 score: 0.8054065444594684\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 3'), FloatProgress(value=0.0, max=166.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d48183a50f404904b746130238ed2052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=57.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ì²EÌ²pÌ²oÌ²cÌ²hÌ² Ì²3\n",
      "Training loss: 0.058956572244572174\n",
      "Validation loss: 0.8368163414364844\n",
      "validation accuracy: 0.8081427774679308\n",
      "f1 score: 0.8079200170633898\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_loss_dict = {}\n",
    "validation_loss_dict = {}\n",
    "\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    #---set the model to training mode\n",
    "    model.train()         \n",
    "    \n",
    "    #counter to keep a sum of loss values across all epochs\n",
    "    total_loss = 0\n",
    "\n",
    "    # Setting up the Progress bar to Moniter the progress of training\n",
    "    pbar = tqdm(train_dloader, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in pbar:\n",
    "        \n",
    "        #---set zero_grad so that gradient values are not accumulated across batches\n",
    "        model.zero_grad()\n",
    "        \n",
    "        #---get the input_ids, attention_masks and labels from each batch int a dictionary to pass as input to the model\n",
    "        temp_batch = tuple(b.to(device) for b in batch)                 #move the batch elements to GPU for faster performance\n",
    "         \n",
    "        input_dict = {'input_ids': temp_batch[0], 'attention_mask': temp_batch[1], 'labels': temp_batch[2]}       \n",
    "\n",
    "        #---Predict outputs using the model\n",
    "        model_output = model(**input_dict)\n",
    "\n",
    "        #---get loss for current batch and add to the total_loss counter\n",
    "        loss = model_output[0]\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        #---compute and clip gradients\n",
    "        loss.backward()                                           #compute gradients via backpropogation\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   #clip gradients to prevent gradient explosion issue\n",
    "\n",
    "        #---Use optimizer and scheduler to tune the model during epochs\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        pbar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "    #---save the model to disk        \n",
    "    #torch.save(model.state_dict(), r'C:\\Users\\Karthik\\Desktop\\Dissertation\\BERT\\Model\\BERT_trained.model')\n",
    "        \n",
    "    #---calculate metrics \n",
    "    loss_train_avg = total_loss/len(train_dloader)            \n",
    "    val_loss, predictions, true_vals, val_accuracy, f1score = model_eval(val_dloader)\n",
    "\n",
    "    training_loss_dict[epoch] = loss_train_avg\n",
    "    validation_loss_dict[epoch] = val_loss\n",
    "    \n",
    "    tqdm.write(\"\\u0332\".join(f'\\nEpoch {epoch}'))\n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'validation accuracy: {val_accuracy}')\n",
    "    tqdm.write(f'f1 score: {f1score}')\n",
    "    \n",
    "torch.save(model, f\"C:\\\\Users\\\\Karthik\\\\Desktop\\\\Dissertation\\\\BERT\\\\Models\\\\BERT_TwitterUSAirline_Batch_{batch_size}_LR_{lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Learning Rate: 2e-5, Batch size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "lr = 2e-5\n",
    "optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dloader)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d15260993d486496bb9deb6356c69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 1'), FloatProgress(value=0.0, max=166.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a336d601abe46c0ba0363dc5bc7298f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=57.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ì²EÌ²pÌ²oÌ²cÌ²hÌ² Ì²1\n",
      "Training loss: 0.07028504623049668\n",
      "Validation loss: 0.9442426318179187\n",
      "validation accuracy: 0.7986614612381484\n",
      "f1 score: 0.7978682975830883\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 2'), FloatProgress(value=0.0, max=166.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6af16d226d44055a542a869fedfe357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=57.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ì²EÌ²pÌ²oÌ²cÌ²hÌ² Ì²2\n",
      "Training loss: 0.04333450746686052\n",
      "Validation loss: 1.044968852348495\n",
      "validation accuracy: 0.8064696040156163\n",
      "f1 score: 0.8061379658628852\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 3'), FloatProgress(value=0.0, max=166.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9c583f15774c8fa3538577ce69d495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=57.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ì²EÌ²pÌ²oÌ²cÌ²hÌ² Ì²3\n",
      "Training loss: 0.026527366777657563\n",
      "Validation loss: 1.1613282686785649\n",
      "validation accuracy: 0.8064696040156163\n",
      "f1 score: 0.8061431531587261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_loss_dict = {}\n",
    "validation_loss_dict = {}\n",
    "\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    #---set the model to training mode\n",
    "    model.train()         \n",
    "    \n",
    "    #counter to keep a sum of loss values across all epochs\n",
    "    total_loss = 0\n",
    "\n",
    "    # Setting up the Progress bar to Moniter the progress of training\n",
    "    pbar = tqdm(train_dloader, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in pbar:\n",
    "        \n",
    "        #---set zero_grad so that gradient values are not accumulated across batches\n",
    "        model.zero_grad()\n",
    "        \n",
    "        #---get the input_ids, attention_masks and labels from each batch int a dictionary to pass as input to the model\n",
    "        temp_batch = tuple(b.to(device) for b in batch)                 #move the batch elements to GPU for faster performance\n",
    "         \n",
    "        input_dict = {'input_ids': temp_batch[0], 'attention_mask': temp_batch[1], 'labels': temp_batch[2]}       \n",
    "\n",
    "        #---Predict outputs using the model\n",
    "        model_output = model(**input_dict)\n",
    "\n",
    "        #---get loss for current batch and add to the total_loss counter\n",
    "        loss = model_output[0]\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        #---compute and clip gradients\n",
    "        loss.backward()                                           #compute gradients via backpropogation\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   #clip gradients to prevent gradient explosion issue\n",
    "\n",
    "        #---Use optimizer and scheduler to tune the model during epochs\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        pbar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "    #---save the model to disk        \n",
    "    #torch.save(model.state_dict(), r'C:\\Users\\Karthik\\Desktop\\Dissertation\\BERT\\Model\\BERT_trained.model')\n",
    "        \n",
    "    #---calculate metrics \n",
    "    loss_train_avg = total_loss/len(train_dloader)            \n",
    "    val_loss, predictions, true_vals, val_accuracy, f1score = model_eval(val_dloader)\n",
    "\n",
    "    training_loss_dict[epoch] = loss_train_avg\n",
    "    validation_loss_dict[epoch] = val_loss\n",
    "    \n",
    "    tqdm.write(\"\\u0332\".join(f'\\nEpoch {epoch}'))\n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'validation accuracy: {val_accuracy}')\n",
    "    tqdm.write(f'f1 score: {f1score}')\n",
    "    \n",
    "torch.save(model, f\"C:\\\\Users\\\\Karthik\\\\Desktop\\\\Dissertation\\\\BERT\\\\Models\\\\BERT_TwitterUSAirline_Batch_{batch_size}_LR_{lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Learning Rate: 1e-5, Batch size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "lr = 1e-5\n",
    "optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dloader)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7fcd0134b154f5cb1c27f9803fd86f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 1'), FloatProgress(value=0.0, max=166.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdb85e9840234d5ba7f51515955c5858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=57.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ì²EÌ²pÌ²oÌ²cÌ²hÌ² Ì²1\n",
      "Training loss: 0.03130397986493028\n",
      "Validation loss: 1.1883599575151478\n",
      "validation accuracy: 0.8031232571109872\n",
      "f1 score: 0.8026091751191283\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 2'), FloatProgress(value=0.0, max=166.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b578c9d50c4fb792c1f7d2fd7fe78f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=57.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ì²EÌ²pÌ²oÌ²cÌ²hÌ² Ì²2\n",
      "Training loss: 0.021463047406925114\n",
      "Validation loss: 1.200392300995749\n",
      "validation accuracy: 0.8025655326268823\n",
      "f1 score: 0.8031848292031342\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 3'), FloatProgress(value=0.0, max=166.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a33c82e174ea48f1996c8837aa019844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=57.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ì²EÌ²pÌ²oÌ²cÌ²hÌ² Ì²3\n",
      "Training loss: 0.01627674711365337\n",
      "Validation loss: 1.3181786202547843\n",
      "validation accuracy: 0.8014500836586727\n",
      "f1 score: 0.8013837129904956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_loss_dict = {}\n",
    "validation_loss_dict = {}\n",
    "\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    #---set the model to training mode\n",
    "    model.train()         \n",
    "    \n",
    "    #counter to keep a sum of loss values across all epochs\n",
    "    total_loss = 0\n",
    "\n",
    "    # Setting up the Progress bar to Moniter the progress of training\n",
    "    pbar = tqdm(train_dloader, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in pbar:\n",
    "        \n",
    "        #---set zero_grad so that gradient values are not accumulated across batches\n",
    "        model.zero_grad()\n",
    "        \n",
    "        #---get the input_ids, attention_masks and labels from each batch int a dictionary to pass as input to the model\n",
    "        temp_batch = tuple(b.to(device) for b in batch)                 #move the batch elements to GPU for faster performance\n",
    "         \n",
    "        input_dict = {'input_ids': temp_batch[0], 'attention_mask': temp_batch[1], 'labels': temp_batch[2]}       \n",
    "\n",
    "        #---Predict outputs using the model\n",
    "        model_output = model(**input_dict)\n",
    "\n",
    "        #---get loss for current batch and add to the total_loss counter\n",
    "        loss = model_output[0]\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        #---compute and clip gradients\n",
    "        loss.backward()                                           #compute gradients via backpropogation\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   #clip gradients to prevent gradient explosion issue\n",
    "\n",
    "        #---Use optimizer and scheduler to tune the model during epochs\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        pbar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "    #---save the model to disk        \n",
    "    #torch.save(model.state_dict(), r'C:\\Users\\Karthik\\Desktop\\Dissertation\\BERT\\Model\\BERT_trained.model')\n",
    "        \n",
    "    #---calculate metrics \n",
    "    loss_train_avg = total_loss/len(train_dloader)            \n",
    "    val_loss, predictions, true_vals, val_accuracy, f1score = model_eval(val_dloader)\n",
    "\n",
    "    training_loss_dict[epoch] = loss_train_avg\n",
    "    validation_loss_dict[epoch] = val_loss\n",
    "    \n",
    "    tqdm.write(\"\\u0332\".join(f'\\nEpoch {epoch}'))\n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'validation accuracy: {val_accuracy}')\n",
    "    tqdm.write(f'f1 score: {f1score}')\n",
    "    \n",
    "torch.save(model, f\"C:\\\\Users\\\\Karthik\\\\Desktop\\\\Dissertation\\\\BERT\\\\Models\\\\BERT_TwitterUSAirline_Batch_{batch_size}_LR_{lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Learning Rate: 4e-5, Batch size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# We Need two different dataloder\n",
    "train_dloader = DataLoader(train_tensor, sampler=RandomSampler(train_tensor), batch_size=batch_size)\n",
    "val_dloader = DataLoader(val_tensor, sampler=RandomSampler(val_tensor), batch_size=batch_size)\n",
    "\n",
    "epochs = 3\n",
    "lr = 4e-5\n",
    "optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dloader)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04451bd7f99047fb991739135fb4e9b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 1'), FloatProgress(value=0.0, max=331.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db4b70e81f341c1a8c65963393759fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=113.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ì²EÌ²pÌ²oÌ²cÌ²hÌ² Ì²1\n",
      "Training loss: 0.11336933263613395\n",
      "Validation loss: 1.40177505878336\n",
      "validation accuracy: 0.7796988287785834\n",
      "f1 score: 0.7808960139235134\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 2'), FloatProgress(value=0.0, max=331.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2559d86946d44958fb470977ef65f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=113.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ì²EÌ²pÌ²oÌ²cÌ²hÌ² Ì²2\n",
      "Training loss: 0.0668226255220361\n",
      "Validation loss: 1.3368073928614705\n",
      "validation accuracy: 0.7986614612381484\n",
      "f1 score: 0.798238938888277\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 3'), FloatProgress(value=0.0, max=331.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "812f11cdf471427aaabe5823d2f29994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=113.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ì²EÌ²pÌ²oÌ²cÌ²hÌ² Ì²3\n",
      "Training loss: 0.031194788838871768\n",
      "Validation loss: 1.4059543909228909\n",
      "validation accuracy: 0.7975460122699386\n",
      "f1 score: 0.7984183227610774\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_loss_dict = {}\n",
    "validation_loss_dict = {}\n",
    "\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    #---set the model to training mode\n",
    "    model.train()         \n",
    "    \n",
    "    #counter to keep a sum of loss values across all epochs\n",
    "    total_loss = 0\n",
    "\n",
    "    # Setting up the Progress bar to Moniter the progress of training\n",
    "    pbar = tqdm(train_dloader, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in pbar:\n",
    "        \n",
    "        #---set zero_grad so that gradient values are not accumulated across batches\n",
    "        model.zero_grad()\n",
    "        \n",
    "        #---get the input_ids, attention_masks and labels from each batch int a dictionary to pass as input to the model\n",
    "        temp_batch = tuple(b.to(device) for b in batch)                 #move the batch elements to GPU for faster performance\n",
    "         \n",
    "        input_dict = {'input_ids': temp_batch[0], 'attention_mask': temp_batch[1], 'labels': temp_batch[2]}       \n",
    "\n",
    "        #---Predict outputs using the model\n",
    "        model_output = model(**input_dict)\n",
    "\n",
    "        #---get loss for current batch and add to the total_loss counter\n",
    "        loss = model_output[0]\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        #---compute and clip gradients\n",
    "        loss.backward()                                           #compute gradients via backpropogation\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   #clip gradients to prevent gradient explosion issue\n",
    "\n",
    "        #---Use optimizer and scheduler to tune the model during epochs\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        pbar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "    #---save the model to disk        \n",
    "    #torch.save(model.state_dict(), r'C:\\Users\\Karthik\\Desktop\\Dissertation\\BERT\\Model\\BERT_trained.model')\n",
    "        \n",
    "    #---calculate metrics \n",
    "    loss_train_avg = total_loss/len(train_dloader)            \n",
    "    val_loss, predictions, true_vals, val_accuracy, f1score = model_eval(val_dloader)\n",
    "\n",
    "    training_loss_dict[epoch] = loss_train_avg\n",
    "    validation_loss_dict[epoch] = val_loss\n",
    "    \n",
    "    tqdm.write(\"\\u0332\".join(f'\\nEpoch {epoch}'))\n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'validation accuracy: {val_accuracy}')\n",
    "    tqdm.write(f'f1 score: {f1score}')\n",
    "    \n",
    "torch.save(model, f\"C:\\\\Users\\\\Karthik\\\\Desktop\\\\Dissertation\\\\BERT\\\\Models\\\\BERT_TwitterUSAirline_Batch_{batch_size}_LR_{lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Learning Rate: 3e-5, Batch size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# We Need two different dataloder\n",
    "train_dloader = DataLoader(train_tensor, sampler=RandomSampler(train_tensor), batch_size=batch_size)\n",
    "val_dloader = DataLoader(val_tensor, sampler=RandomSampler(val_tensor), batch_size=batch_size)\n",
    "\n",
    "epochs = 3\n",
    "lr = 3e-5\n",
    "optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dloader)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82446ff0cc5e482d9f108992584aa1e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 1'), FloatProgress(value=0.0, max=331.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec84304bb0e48c5af7e66528336e883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=113.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ì²EÌ²pÌ²oÌ²cÌ²hÌ² Ì²1\n",
      "Training loss: 0.08165258794818622\n",
      "Validation loss: 1.3088159010141873\n",
      "validation accuracy: 0.7847183491355271\n",
      "f1 score: 0.7853352503738621\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 2'), FloatProgress(value=0.0, max=331.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a16d17b182f4db1a2b297281289cae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=113.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ì²EÌ²pÌ²oÌ²cÌ²hÌ² Ì²2\n",
      "Training loss: 0.0324793941697181\n",
      "Validation loss: 1.4275403127892754\n",
      "validation accuracy: 0.7947573898494144\n",
      "f1 score: 0.7940687884577671\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 3'), FloatProgress(value=0.0, max=331.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330dab431959441cb9d44c528197ec5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=113.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ì²EÌ²pÌ²oÌ²cÌ²hÌ² Ì²3\n",
      "Training loss: 0.017855956626546376\n",
      "Validation loss: 1.5000386506237142\n",
      "validation accuracy: 0.7975460122699386\n",
      "f1 score: 0.7974403846671503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_loss_dict = {}\n",
    "validation_loss_dict = {}\n",
    "\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    #---set the model to training mode\n",
    "    model.train()         \n",
    "    \n",
    "    #counter to keep a sum of loss values across all epochs\n",
    "    total_loss = 0\n",
    "\n",
    "    # Setting up the Progress bar to Moniter the progress of training\n",
    "    pbar = tqdm(train_dloader, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in pbar:\n",
    "        \n",
    "        #---set zero_grad so that gradient values are not accumulated across batches\n",
    "        model.zero_grad()\n",
    "        \n",
    "        #---get the input_ids, attention_masks and labels from each batch int a dictionary to pass as input to the model\n",
    "        temp_batch = tuple(b.to(device) for b in batch)                 #move the batch elements to GPU for faster performance\n",
    "         \n",
    "        input_dict = {'input_ids': temp_batch[0], 'attention_mask': temp_batch[1], 'labels': temp_batch[2]}       \n",
    "\n",
    "        #---Predict outputs using the model\n",
    "        model_output = model(**input_dict)\n",
    "\n",
    "        #---get loss for current batch and add to the total_loss counter\n",
    "        loss = model_output[0]\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        #---compute and clip gradients\n",
    "        loss.backward()                                           #compute gradients via backpropogation\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   #clip gradients to prevent gradient explosion issue\n",
    "\n",
    "        #---Use optimizer and scheduler to tune the model during epochs\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        pbar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "    #---save the model to disk        \n",
    "    #torch.save(model.state_dict(), r'C:\\Users\\Karthik\\Desktop\\Dissertation\\BERT\\Model\\BERT_trained.model')\n",
    "        \n",
    "    #---calculate metrics \n",
    "    loss_train_avg = total_loss/len(train_dloader)            \n",
    "    val_loss, predictions, true_vals, val_accuracy, f1score = model_eval(val_dloader)\n",
    "\n",
    "    training_loss_dict[epoch] = loss_train_avg\n",
    "    validation_loss_dict[epoch] = val_loss\n",
    "    \n",
    "    tqdm.write(\"\\u0332\".join(f'\\nEpoch {epoch}'))\n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'validation accuracy: {val_accuracy}')\n",
    "    tqdm.write(f'f1 score: {f1score}')\n",
    "    \n",
    "torch.save(model, f\"C:\\\\Users\\\\Karthik\\\\Desktop\\\\Dissertation\\\\BERT\\\\Models\\\\BERT_TwitterUSAirline_Batch_{batch_size}_LR_{lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Learning Rate: 2e-5, Batch size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# We Need two different dataloder\n",
    "train_dloader = DataLoader(train_tensor, sampler=RandomSampler(train_tensor), batch_size=batch_size)\n",
    "val_dloader = DataLoader(val_tensor, sampler=RandomSampler(val_tensor), batch_size=batch_size)\n",
    "\n",
    "epochs = 3\n",
    "lr = 2e-5\n",
    "optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dloader)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a5e7ff470584783887db5534b656221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 1'), FloatProgress(value=0.0, max=331.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fcc8f68f6a942a6bf26b91bd59cecf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=113.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ì²EÌ²pÌ²oÌ²cÌ²hÌ² Ì²1\n",
      "Training loss: 0.03474160280563831\n",
      "Validation loss: 1.4872182012655781\n",
      "validation accuracy: 0.7941996653653095\n",
      "f1 score: 0.7946660240760588\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 2'), FloatProgress(value=0.0, max=331.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74eec2dafa9f40d0ac95f158e0139916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=113.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ì²EÌ²pÌ²oÌ²cÌ²hÌ² Ì²2\n",
      "Training loss: 0.019447657142438204\n",
      "Validation loss: 1.5265365434514073\n",
      "validation accuracy: 0.7908533184606804\n",
      "f1 score: 0.7897338729018218\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 3'), FloatProgress(value=0.0, max=331.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e6560cfcad42589bfdb166f5ba33bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=113.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ì²EÌ²pÌ²oÌ²cÌ²hÌ² Ì²3\n",
      "Training loss: 0.016787038144827512\n",
      "Validation loss: 1.5583263466802253\n",
      "validation accuracy: 0.7919687674288901\n",
      "f1 score: 0.7911522671160824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_loss_dict = {}\n",
    "validation_loss_dict = {}\n",
    "\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    #---set the model to training mode\n",
    "    model.train()         \n",
    "    \n",
    "    #counter to keep a sum of loss values across all epochs\n",
    "    total_loss = 0\n",
    "\n",
    "    # Setting up the Progress bar to Moniter the progress of training\n",
    "    pbar = tqdm(train_dloader, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in pbar:\n",
    "        \n",
    "        #---set zero_grad so that gradient values are not accumulated across batches\n",
    "        model.zero_grad()\n",
    "        \n",
    "        #---get the input_ids, attention_masks and labels from each batch int a dictionary to pass as input to the model\n",
    "        temp_batch = tuple(b.to(device) for b in batch)                 #move the batch elements to GPU for faster performance\n",
    "         \n",
    "        input_dict = {'input_ids': temp_batch[0], 'attention_mask': temp_batch[1], 'labels': temp_batch[2]}       \n",
    "\n",
    "        #---Predict outputs using the model\n",
    "        model_output = model(**input_dict)\n",
    "\n",
    "        #---get loss for current batch and add to the total_loss counter\n",
    "        loss = model_output[0]\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        #---compute and clip gradients\n",
    "        loss.backward()                                           #compute gradients via backpropogation\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   #clip gradients to prevent gradient explosion issue\n",
    "\n",
    "        #---Use optimizer and scheduler to tune the model during epochs\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        pbar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "    #---save the model to disk        \n",
    "    #torch.save(model.state_dict(), r'C:\\Users\\Karthik\\Desktop\\Dissertation\\BERT\\Model\\BERT_trained.model')\n",
    "        \n",
    "    #---calculate metrics \n",
    "    loss_train_avg = total_loss/len(train_dloader)            \n",
    "    val_loss, predictions, true_vals, val_accuracy, f1score = model_eval(val_dloader)\n",
    "\n",
    "    training_loss_dict[epoch] = loss_train_avg\n",
    "    validation_loss_dict[epoch] = val_loss\n",
    "    \n",
    "    tqdm.write(\"\\u0332\".join(f'\\nEpoch {epoch}'))\n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'validation accuracy: {val_accuracy}')\n",
    "    tqdm.write(f'f1 score: {f1score}')\n",
    "    \n",
    "torch.save(model, f\"C:\\\\Users\\\\Karthik\\\\Desktop\\\\Dissertation\\\\BERT\\\\Models\\\\BERT_TwitterUSAirline_Batch_{batch_size}_LR_{lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Learning Rate: 1e-5, Batch size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# We Need two different dataloder\n",
    "train_dloader = DataLoader(train_tensor, sampler=RandomSampler(train_tensor), batch_size=batch_size)\n",
    "val_dloader = DataLoader(val_tensor, sampler=RandomSampler(val_tensor), batch_size=batch_size)\n",
    "\n",
    "epochs = 3\n",
    "lr = 1e-5\n",
    "optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dloader)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d82467d9214479a7ed544982136c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 1'), FloatProgress(value=0.0, max=331.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91cb3e3e6684ccab9909455dc25535d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=113.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ì²EÌ²pÌ²oÌ²cÌ²hÌ² Ì²1\n",
      "Training loss: 0.017547980702383774\n",
      "Validation loss: 1.6495554934025782\n",
      "validation accuracy: 0.7930842163970998\n",
      "f1 score: 0.7924047169152861\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 2'), FloatProgress(value=0.0, max=331.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa022b3140f3400e98dde940bec643c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=113.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ì²EÌ²pÌ²oÌ²cÌ²hÌ² Ì²2\n",
      "Training loss: 0.011069880382806235\n",
      "Validation loss: 1.6537227041228701\n",
      "validation accuracy: 0.7969882877858337\n",
      "f1 score: 0.7966815209923267\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 3'), FloatProgress(value=0.0, max=331.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46bde19f32d463892136977bc17d5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=113.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ì²EÌ²pÌ²oÌ²cÌ²hÌ² Ì²3\n",
      "Training loss: 0.009143708631279527\n",
      "Validation loss: 1.6660743541143404\n",
      "validation accuracy: 0.7975460122699386\n",
      "f1 score: 0.7973060814107703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_loss_dict = {}\n",
    "validation_loss_dict = {}\n",
    "\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    #---set the model to training mode\n",
    "    model.train()         \n",
    "    \n",
    "    #counter to keep a sum of loss values across all epochs\n",
    "    total_loss = 0\n",
    "\n",
    "    # Setting up the Progress bar to Moniter the progress of training\n",
    "    pbar = tqdm(train_dloader, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in pbar:\n",
    "        \n",
    "        #---set zero_grad so that gradient values are not accumulated across batches\n",
    "        model.zero_grad()\n",
    "        \n",
    "        #---get the input_ids, attention_masks and labels from each batch int a dictionary to pass as input to the model\n",
    "        temp_batch = tuple(b.to(device) for b in batch)                 #move the batch elements to GPU for faster performance\n",
    "         \n",
    "        input_dict = {'input_ids': temp_batch[0], 'attention_mask': temp_batch[1], 'labels': temp_batch[2]}       \n",
    "\n",
    "        #---Predict outputs using the model\n",
    "        model_output = model(**input_dict)\n",
    "\n",
    "        #---get loss for current batch and add to the total_loss counter\n",
    "        loss = model_output[0]\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        #---compute and clip gradients\n",
    "        loss.backward()                                           #compute gradients via backpropogation\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   #clip gradients to prevent gradient explosion issue\n",
    "\n",
    "        #---Use optimizer and scheduler to tune the model during epochs\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        pbar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "    #---save the model to disk        \n",
    "    #torch.save(model.state_dict(), r'C:\\Users\\Karthik\\Desktop\\Dissertation\\BERT\\Model\\BERT_trained.model')\n",
    "        \n",
    "    #---calculate metrics \n",
    "    loss_train_avg = total_loss/len(train_dloader)            \n",
    "    val_loss, predictions, true_vals, val_accuracy, f1score = model_eval(val_dloader)\n",
    "\n",
    "    training_loss_dict[epoch] = loss_train_avg\n",
    "    validation_loss_dict[epoch] = val_loss\n",
    "    \n",
    "    tqdm.write(\"\\u0332\".join(f'\\nEpoch {epoch}'))\n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'validation accuracy: {val_accuracy}')\n",
    "    tqdm.write(f'f1 score: {f1score}')\n",
    "    \n",
    "torch.save(model, f\"C:\\\\Users\\\\Karthik\\\\Desktop\\\\Dissertation\\\\BERT\\\\Models\\\\BERT_TwitterUSAirline_Batch_{batch_size}_LR_{lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predict with the best model\n",
    "- *Batch size of 32 and learning rate of 4e-4 gave the best results*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Load the selected model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reqd_model = torch.load(r\"C:\\Users\\Karthik\\Desktop\\Dissertation\\BERT\\Models\\BERT_TwitterUSAirline_Batch_32_LR_4e-05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "reqd_model.to(device) \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Prepare the datasets to be used in the BERT model for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 Clean the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tick, tick_df in reddit_df_dict.items():\n",
    "#     print(tick, tick_df)\n",
    "    tick_df['cleaned_body'] = tick_df['body'].apply(lambda x: text_preprocessing(x))\n",
    "    tick_df['LEN'] = tick_df.cleaned_body.str.len()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tick, tick_df in twitter_df_dict.items():\n",
    "    tick_df['cleaned_body'] = tick_df['cleaned_content'].apply(lambda x: text_preprocessing(x))\n",
    "    tick_df['LEN'] = tick_df.cleaned_body.str.len()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 Handle records where text is greater than a length of 512\n",
    "- *BERT can only handle a max length of 512*\n",
    "- *For each comment > length of 512, break the comment into multiple sets of 512*\n",
    "- *After breaking, create a new record for each of the broken parts and append to the original dataframe. Since we take the average sentiment scores, appending new rows with the same dates will not affect the data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=512\n",
    "def ffn(xseries):\n",
    "    list_of_lists = []\n",
    "        \n",
    "    if xseries['LEN']>512:\n",
    "        parts = [xseries['cleaned_body'][i:i+n] for i in range(0, xseries['LEN'], n)]\n",
    "    \n",
    "        counter=0\n",
    "        for i in parts:\n",
    "            list_series = []\n",
    "            list_series.append(xseries['body'])\n",
    "            list_series.append(xseries['created_utc'])\n",
    "            list_series.append(xseries['id'])\n",
    "            list_series.append(xseries['top'])\n",
    "            list_series.append(xseries['year'])\n",
    "            list_series.append(xseries['month'])\n",
    "            list_series.append(xseries['day'])\n",
    "            list_series.append(xseries['ticker'])\n",
    "            list_series.append(xseries['YearMonDay'])\n",
    "            list_series.append(i)\n",
    "            counter+=1\n",
    "            list_series.append(counter)\n",
    "            \n",
    "            list_of_lists.append(list_series)\n",
    "    \n",
    "        return list_of_lists\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tick, tick_df in reddit_df_dict.items():\n",
    "    tick_df['mltp'] = tick_df.apply(ffn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tick, tick_df in twitter_df_dict.items():\n",
    "    tick_df['mltp'] = tick_df.apply(ffn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tick, tick_df in reddit_df_dict.items():\n",
    "    addition_list = tick_df.query('mltp != 0')['mltp']\n",
    "    \n",
    "    if len(addition_list)==0:\n",
    "        continue\n",
    "    \n",
    "    ind_additions = []\n",
    "\n",
    "    for list_of_sentences in addition_list:\n",
    "        for sentence_part in list_of_sentences:\n",
    "            ind_additions.append(sentence_part)\n",
    "    \n",
    "    \n",
    "    df_additions = pd.DataFrame(ind_additions, columns=['body','created_utc','id','top','year','month','day','ticker','YearMonDay','cleaned_body','LEN'])\n",
    "    \n",
    "    tick_df.drop('mltp', axis=1, inplace=True)\n",
    "    tick_df.drop(tick_df[tick_df.LEN > 512].index, inplace = True)\n",
    "    tick_df = tick_df.append(df_additions, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tick, tick_df in twitter_df_dict.items():\n",
    "    addition_list = tick_df.query('mltp != 0')['mltp']\n",
    "    \n",
    "    if len(addition_list)==0:\n",
    "        continue\n",
    "    \n",
    "    ind_additions = []\n",
    "\n",
    "    for list_of_sentences in addition_list:\n",
    "        for sentence_part in list_of_sentences:\n",
    "            ind_additions.append(sentence_part)\n",
    "    \n",
    "    \n",
    "    df_additions = pd.DataFrame(ind_additions, columns=['body','created_utc','id','top','year','month','day','ticker','YearMonDay','cleaned_body','LEN'])\n",
    "    \n",
    "    tick_df.drop('mltp', axis=1, inplace=True)\n",
    "    tick_df.drop(tick_df[tick_df.LEN > 512].index, inplace = True)\n",
    "    tick_df = tick_df.append(df_additions, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Make sure there is no data where comment length is greater than 512*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [body, created_utc, id, top, year, month, day, ticker, YearMonDay, cleaned_body, LEN]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [body, created_utc, id, top, year, month, day, ticker, YearMonDay, cleaned_body, LEN]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [body, created_utc, id, top, year, month, day, ticker, YearMonDay, cleaned_body, LEN]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [body, created_utc, id, top, year, month, day, ticker, YearMonDay, cleaned_body, LEN]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [body, created_utc, id, top, year, month, day, ticker, YearMonDay, cleaned_body, LEN]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "for tick, tick_df in reddit_df_dict.items():\n",
    "    print(tick_df.query('LEN > 512'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [date, content, likeCount, lang, cleaned_content, year, month, day, ticker, YearMonDay, cleaned_body, LEN, mltp]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [date, content, likeCount, lang, cleaned_content, year, month, day, ticker, YearMonDay, cleaned_body, LEN, mltp]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [date, content, likeCount, lang, cleaned_content, year, month, day, ticker, YearMonDay, cleaned_body, LEN, mltp]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [date, content, likeCount, lang, cleaned_content, year, month, day, ticker, YearMonDay, cleaned_body, LEN, mltp]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [date, content, likeCount, lang, cleaned_content, year, month, day, ticker, YearMonDay, cleaned_body, LEN, mltp]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "for tick, tick_df in twitter_df_dict.items():\n",
    "    print(tick_df.query('LEN > 512'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3 Define a funtion to prepare reddit and twitter data for model prediction\n",
    "- *The data should be tokenized and encoded and then tensor datasets must be created from them to be passed to the BERT model*\n",
    "- *We will create a dictionary for reddit and twitter to hold the tensor datasets respectively*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Initialize bert-base-uncased tokenizer and select the option to convert all text to lowercase\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "def dataloader_fnc(text, MAX_LEN):\n",
    "    input_id_list = []\n",
    "    attn_mask_list = []\n",
    "    \n",
    "    for line in text:\n",
    "        #--Tokenize and encode the cleaned text (we do not return_tensors here since our tensor should have all the lines)\n",
    "        data_enc = tokenizer.encode_plus(line, \n",
    "                                         add_special_tokens=True, \n",
    "                                         return_attention_mask=True, \n",
    "                                         pad_to_max_length=True, \n",
    "                                         max_length=MAX_LEN\n",
    "                                         )\n",
    "        \n",
    "        #--get the input_ids and attention_mask values\n",
    "        input_id_list.append(data_enc.get(\"input_ids\"))\n",
    "        attn_mask_list.append(data_enc.get(\"attention_mask\"))\n",
    "    \n",
    "    tensor_dataset = TensorDataset(torch.tensor(input_id_list), torch.tensor(attn_mask_list) )\n",
    "    dataloader = DataLoader(tensor_dataset, sampler=SequentialSampler(tensor_dataset), batch_size=32)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2184: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "reddit_dataloader_dict = {tick: dataloader_fnc(tick_df.cleaned_body, tick_df.cleaned_body.str.len().max())  for tick, tick_df in reddit_df_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AMC': <torch.utils.data.dataloader.DataLoader at 0x1b88e9e7700>,\n",
       " 'DKNG': <torch.utils.data.dataloader.DataLoader at 0x1b88e9cdaf0>,\n",
       " 'TSLA': <torch.utils.data.dataloader.DataLoader at 0x1b8647ae2e0>,\n",
       " 'AMD': <torch.utils.data.dataloader.DataLoader at 0x1b88e9cd400>,\n",
       " 'BABA': <torch.utils.data.dataloader.DataLoader at 0x1b91da9a580>}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_dataloader_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_dataloader_dict = {tick: dataloader_fnc(tick_df.cleaned_body, tick_df.cleaned_body.str.len().max())  for tick, tick_df in twitter_df_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AMC': <torch.utils.data.dataloader.DataLoader at 0x1b88e9cdc40>,\n",
       " 'DKNG': <torch.utils.data.dataloader.DataLoader at 0x1b91da9a9a0>,\n",
       " 'TSLA': <torch.utils.data.dataloader.DataLoader at 0x1b91da9adf0>,\n",
       " 'AMD': <torch.utils.data.dataloader.DataLoader at 0x1b91da9a3d0>,\n",
       " 'BABA': <torch.utils.data.dataloader.DataLoader at 0x1b86a7752e0>}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_dataloader_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Define a prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(bert_model, data):\n",
    "    #--put the model in eval mode\n",
    "    bert_model.eval()\n",
    "    \n",
    "    #--predict and collect output logits in a list\n",
    "    output_logit_list = []\n",
    "    \n",
    "    for batch in data:\n",
    "        temp_batch = tuple(b.to(device) for b in batch)                 #move the batch elements to GPU for faster performance\n",
    "        input_id_tensor = temp_batch[0]\n",
    "        attn_mask_tensor = temp_batch[1]\n",
    "#         print(input_id_tensor)\n",
    "#         print(attn_mask_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output_logits = bert_model(input_id_tensor, attn_mask_tensor)\n",
    "            output_logit_list.append(output_logits[0])\n",
    "    \n",
    "    #--concatenate logits across batches and apply softmax\n",
    "    complete_logits = torch.cat(output_logit_list, dim=0)\n",
    "    softmax_probs = Func.softmax(complete_logits, dim=1).cpu().numpy() #convert to numpy\n",
    "    \n",
    "    return softmax_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Predict sentiments for reddit data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.1 Prediction for reddit comments\n",
    "- *Use the model to get positive and negative probabilities for each reddit comment*\n",
    "- *Group by date and compute average positive, negative and neutral scores for each day*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tick, tick_df in reddit_df_dict.items():\n",
    "    pred_df = pd.DataFrame(prediction(reqd_model, reddit_dataloader_dict[tick]), columns=['neutral', 'positive', 'negative'])\n",
    "    reddit_df_dict[tick] = pd.concat([tick_df, pred_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ticker YearMonDay   neutral  positive  negative\n",
      "0      AMC    20Aug05  0.091782  0.225708  0.682509\n",
      "1      AMC    20Aug13  0.973933  0.003902  0.022165\n",
      "2      AMC    20Aug17  0.639910  0.011118  0.348972\n",
      "3      AMC    20Aug19  0.876112  0.099524  0.024364\n",
      "4      AMC    20Aug25  0.659872  0.315126  0.025002\n",
      "..     ...        ...       ...       ...       ...\n",
      "175    AMC    21May26  0.605147  0.153711  0.241142\n",
      "176    AMC    21May27  0.557445  0.174559  0.267996\n",
      "177    AMC    21May28  0.559392  0.165391  0.275217\n",
      "178    AMC    21May29  0.324422  0.259156  0.416423\n",
      "179    AMC    21May30  0.982039  0.008089  0.009872\n",
      "\n",
      "[180 rows x 5 columns]\n",
      "    ticker YearMonDay   neutral  positive  negative\n",
      "0     DKNG    20Aug03  0.841356  0.044665  0.113979\n",
      "1     DKNG    20Aug05  0.756529  0.040376  0.203095\n",
      "2     DKNG    20Aug06  0.495894  0.013155  0.490951\n",
      "3     DKNG    20Aug10  0.292190  0.031255  0.676555\n",
      "4     DKNG    20Aug11  0.556901  0.009579  0.433521\n",
      "..     ...        ...       ...       ...       ...\n",
      "231   DKNG    21May24  0.540585  0.219709  0.239706\n",
      "232   DKNG    21May25  0.369158  0.201787  0.429055\n",
      "233   DKNG    21May26  0.411554  0.106010  0.482436\n",
      "234   DKNG    21May27  0.680789  0.136315  0.182896\n",
      "235   DKNG    21May28  0.927313  0.054552  0.018135\n",
      "\n",
      "[236 rows x 5 columns]\n",
      "    ticker YearMonDay   neutral  positive  negative\n",
      "0     TSLA    20Aug03  0.567100  0.181165  0.251736\n",
      "1     TSLA    20Aug05  0.809800  0.136642  0.053559\n",
      "2     TSLA    20Aug06  0.582649  0.124612  0.292739\n",
      "3     TSLA    20Aug10  0.525351  0.107985  0.366664\n",
      "4     TSLA    20Aug11  0.512256  0.184105  0.303639\n",
      "..     ...        ...       ...       ...       ...\n",
      "256   TSLA    21May24  0.598573  0.142049  0.259377\n",
      "257   TSLA    21May25  0.683956  0.155082  0.160962\n",
      "258   TSLA    21May26  0.650852  0.168264  0.180884\n",
      "259   TSLA    21May27  0.549439  0.177225  0.273336\n",
      "260   TSLA    21May28  0.630860  0.203621  0.165519\n",
      "\n",
      "[261 rows x 5 columns]\n",
      "    ticker YearMonDay   neutral  positive  negative\n",
      "0      AMD    20Aug03  0.494828  0.013387  0.491785\n",
      "1      AMD    20Aug05  0.584020  0.106240  0.309741\n",
      "2      AMD    20Aug06  0.475436  0.208960  0.315604\n",
      "3      AMD    20Aug10  0.590553  0.101755  0.307692\n",
      "4      AMD    20Aug11  0.455445  0.017979  0.526576\n",
      "..     ...        ...       ...       ...       ...\n",
      "251    AMD    21May25  0.632962  0.078760  0.288279\n",
      "252    AMD    21May26  0.487418  0.102014  0.410567\n",
      "253    AMD    21May27  0.547568  0.088268  0.364164\n",
      "254    AMD    21May28  0.498769  0.281527  0.219704\n",
      "255    AMD    21May29  0.144648  0.046015  0.809337\n",
      "\n",
      "[256 rows x 5 columns]\n",
      "    ticker YearMonDay   neutral  positive  negative\n",
      "0     BABA    20Aug03  0.644130  0.145981  0.209888\n",
      "1     BABA    20Aug05  0.916217  0.070077  0.013706\n",
      "2     BABA    20Aug06  0.908196  0.074731  0.017073\n",
      "3     BABA    20Aug10  0.713519  0.018295  0.268186\n",
      "4     BABA    20Aug11  0.660402  0.023118  0.316480\n",
      "..     ...        ...       ...       ...       ...\n",
      "230   BABA    21May24  0.486703  0.134951  0.378346\n",
      "231   BABA    21May25  0.631392  0.065153  0.303455\n",
      "232   BABA    21May26  0.443489  0.060896  0.495615\n",
      "233   BABA    21May27  0.646457  0.197622  0.155920\n",
      "234   BABA    21May28  0.681390  0.164546  0.154064\n",
      "\n",
      "[235 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "for tick, tick_df in reddit_df_dict.items():\n",
    "    tick_grouped = tick_df.groupby(['ticker', 'YearMonDay'])[['neutral', 'positive', 'negative']].mean().reset_index()\n",
    "    tick_grouped.to_pickle(f\"C:\\\\Users\\\\Karthik\\\\Desktop\\\\Dissertation\\\\Final_dfs\\\\reddit_{tick}_BERT_TwitterAirline_Even.pkl\")\n",
    "    print(tick_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2 Prediction for twitter comments\n",
    "- *Use the model to get positive and negative probabilities for each twitter comment*\n",
    "- *Group by date and compute average positive, negative and neutral scores for each day*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tick, tick_df in twitter_df_dict.items():\n",
    "    pred_df = pd.DataFrame(prediction(reqd_model, twitter_dataloader_dict[tick]), columns=['neutral', 'positive', 'negative'])\n",
    "    twitter_df_dict[tick] = pd.concat([tick_df, pred_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ticker YearMonDay   neutral  positive  negative\n",
      "0      AMC    20Aug01  0.450005  0.311928  0.238067\n",
      "1      AMC    20Aug02  0.599266  0.081191  0.319544\n",
      "2      AMC    20Aug03  0.734386  0.050787  0.214826\n",
      "3      AMC    20Aug04  0.790502  0.089713  0.119785\n",
      "4      AMC    20Aug05  0.677792  0.116865  0.205342\n",
      "..     ...        ...       ...       ...       ...\n",
      "482    AMC    21Sep26  0.496433  0.215175  0.288392\n",
      "483    AMC    21Sep27  0.465371  0.196891  0.337738\n",
      "484    AMC    21Sep28  0.417595  0.130639  0.451766\n",
      "485    AMC    21Sep29  0.458247  0.188060  0.353693\n",
      "486    AMC    21Sep30  0.453878  0.223223  0.322899\n",
      "\n",
      "[487 rows x 5 columns]\n",
      "    ticker YearMonDay   neutral  positive  negative\n",
      "0     DKNG    20Aug01  0.583589  0.219671  0.196740\n",
      "1     DKNG    20Aug02  0.449766  0.191134  0.359100\n",
      "2     DKNG    20Aug03  0.511297  0.093189  0.395514\n",
      "3     DKNG    20Aug04  0.597515  0.155424  0.247060\n",
      "4     DKNG    20Aug05  0.750923  0.073731  0.175345\n",
      "..     ...        ...       ...       ...       ...\n",
      "482   DKNG    21Sep26  0.797843  0.094472  0.107684\n",
      "483   DKNG    21Sep27  0.599195  0.139872  0.260933\n",
      "484   DKNG    21Sep28  0.627612  0.127781  0.244607\n",
      "485   DKNG    21Sep29  0.637146  0.111351  0.251503\n",
      "486   DKNG    21Sep30  0.594008  0.121808  0.284185\n",
      "\n",
      "[487 rows x 5 columns]\n",
      "    ticker YearMonDay   neutral  positive  negative\n",
      "0     TSLA    20Aug01  0.556257  0.156017  0.287726\n",
      "1     TSLA    20Aug02  0.576985  0.183644  0.239371\n",
      "2     TSLA    20Aug03  0.536352  0.160211  0.303436\n",
      "3     TSLA    20Aug04  0.543042  0.140153  0.316804\n",
      "4     TSLA    20Aug05  0.540466  0.168389  0.291145\n",
      "..     ...        ...       ...       ...       ...\n",
      "482   TSLA    21Sep26  0.546167  0.198430  0.255403\n",
      "483   TSLA    21Sep27  0.551046  0.201317  0.247637\n",
      "484   TSLA    21Sep28  0.573442  0.156690  0.269868\n",
      "485   TSLA    21Sep29  0.513024  0.170364  0.316613\n",
      "486   TSLA    21Sep30  0.537713  0.158011  0.304276\n",
      "\n",
      "[487 rows x 5 columns]\n",
      "    ticker YearMonDay   neutral  positive  negative\n",
      "0      AMD    20Aug05  0.761809  0.200659  0.037533\n",
      "1      AMD    20Aug08  0.730011  0.259581  0.010409\n",
      "2      AMD    20Dec03  0.923622  0.058945  0.017433\n",
      "3      AMD    20Dec04  0.786348  0.179898  0.033754\n",
      "4      AMD    20Dec09  0.738550  0.251427  0.010023\n",
      "..     ...        ...       ...       ...       ...\n",
      "237    AMD    21Sep26  0.683440  0.156861  0.159699\n",
      "238    AMD    21Sep27  0.576596  0.240045  0.183359\n",
      "239    AMD    21Sep28  0.646228  0.116786  0.236985\n",
      "240    AMD    21Sep29  0.690471  0.101946  0.207583\n",
      "241    AMD    21Sep30  0.624135  0.198729  0.177135\n",
      "\n",
      "[242 rows x 5 columns]\n",
      "    ticker YearMonDay   neutral  positive  negative\n",
      "0     BABA    20Aug01  0.721527  0.080642  0.197830\n",
      "1     BABA    20Aug02  0.598508  0.198813  0.202678\n",
      "2     BABA    20Aug03  0.598261  0.192071  0.209668\n",
      "3     BABA    20Aug04  0.677058  0.143654  0.179287\n",
      "4     BABA    20Aug05  0.725039  0.135145  0.139816\n",
      "..     ...        ...       ...       ...       ...\n",
      "482   BABA    21Sep26  0.559959  0.126927  0.313114\n",
      "483   BABA    21Sep27  0.659376  0.100793  0.239831\n",
      "484   BABA    21Sep28  0.615119  0.127082  0.257799\n",
      "485   BABA    21Sep29  0.633783  0.097332  0.268884\n",
      "486   BABA    21Sep30  0.656489  0.083950  0.259561\n",
      "\n",
      "[487 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "for tick, tick_df in twitter_df_dict.items():\n",
    "    tick_grouped = tick_df.groupby(['ticker', 'YearMonDay'])[['neutral', 'positive', 'negative']].mean().reset_index()\n",
    "    tick_grouped.to_pickle(f\"C:\\\\Users\\\\Karthik\\\\Desktop\\\\Dissertation\\\\Final_dfs\\\\twitter_{tick}_BERT_TwitterAirline_Even.pkl\")\n",
    "    print(tick_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.3 Calculate combined predictions by combining both reddit and twitter sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMC\n",
      "    ticker YearMonDay   neutral  positive  negative\n",
      "0      AMC    20Aug01  0.450005  0.311928  0.238067\n",
      "1      AMC    20Aug02  0.599266  0.081191  0.319544\n",
      "2      AMC    20Aug03  0.734386  0.050787  0.214826\n",
      "3      AMC    20Aug04  0.790502  0.089713  0.119785\n",
      "4      AMC    20Aug05  0.624519  0.126760  0.248721\n",
      "..     ...        ...       ...       ...       ...\n",
      "488    AMC    21Sep26  0.496433  0.215175  0.288392\n",
      "489    AMC    21Sep27  0.465371  0.196891  0.337738\n",
      "490    AMC    21Sep28  0.417595  0.130639  0.451766\n",
      "491    AMC    21Sep29  0.458247  0.188060  0.353693\n",
      "492    AMC    21Sep30  0.453878  0.223223  0.322899\n",
      "\n",
      "[493 rows x 5 columns]\n",
      "DKNG\n",
      "    ticker YearMonDay   neutral  positive  negative\n",
      "0     DKNG    20Aug01  0.583589  0.219671  0.196740\n",
      "1     DKNG    20Aug02  0.449766  0.191134  0.359100\n",
      "2     DKNG    20Aug03  0.519760  0.091945  0.388295\n",
      "3     DKNG    20Aug04  0.597515  0.155424  0.247060\n",
      "4     DKNG    20Aug05  0.751022  0.073146  0.175832\n",
      "..     ...        ...       ...       ...       ...\n",
      "500   DKNG    21Sep26  0.797843  0.094472  0.107684\n",
      "501   DKNG    21Sep27  0.599195  0.139872  0.260933\n",
      "502   DKNG    21Sep28  0.627612  0.127781  0.244607\n",
      "503   DKNG    21Sep29  0.637146  0.111351  0.251503\n",
      "504   DKNG    21Sep30  0.594008  0.121808  0.284185\n",
      "\n",
      "[505 rows x 5 columns]\n",
      "TSLA\n",
      "    ticker YearMonDay   neutral  positive  negative\n",
      "0     TSLA    20Aug01  0.556257  0.156017  0.287726\n",
      "1     TSLA    20Aug02  0.576985  0.183644  0.239371\n",
      "2     TSLA    20Aug03  0.538181  0.161458  0.300361\n",
      "3     TSLA    20Aug04  0.543042  0.140153  0.316804\n",
      "4     TSLA    20Aug05  0.544648  0.167896  0.287456\n",
      "..     ...        ...       ...       ...       ...\n",
      "500   TSLA    21Sep26  0.546167  0.198430  0.255403\n",
      "501   TSLA    21Sep27  0.551046  0.201317  0.247637\n",
      "502   TSLA    21Sep28  0.573442  0.156690  0.269868\n",
      "503   TSLA    21Sep29  0.513024  0.170364  0.316613\n",
      "504   TSLA    21Sep30  0.537713  0.158011  0.304276\n",
      "\n",
      "[505 rows x 5 columns]\n",
      "AMD\n",
      "    ticker YearMonDay   neutral  positive  negative\n",
      "0      AMD    20Aug03  0.494828  0.013387  0.491785\n",
      "1      AMD    20Aug05  0.600952  0.115232  0.283816\n",
      "2      AMD    20Aug06  0.475436  0.208960  0.315604\n",
      "3      AMD    20Aug08  0.730011  0.259581  0.010409\n",
      "4      AMD    20Aug10  0.590553  0.101755  0.307692\n",
      "..     ...        ...       ...       ...       ...\n",
      "384    AMD    21Sep26  0.683440  0.156861  0.159699\n",
      "385    AMD    21Sep27  0.576596  0.240045  0.183359\n",
      "386    AMD    21Sep28  0.646228  0.116786  0.236985\n",
      "387    AMD    21Sep29  0.690471  0.101946  0.207583\n",
      "388    AMD    21Sep30  0.624135  0.198729  0.177135\n",
      "\n",
      "[389 rows x 5 columns]\n",
      "BABA\n",
      "    ticker YearMonDay   neutral  positive  negative\n",
      "0     BABA    20Aug01  0.721527  0.080642  0.197830\n",
      "1     BABA    20Aug02  0.598508  0.198813  0.202678\n",
      "2     BABA    20Aug03  0.600754  0.189567  0.209680\n",
      "3     BABA    20Aug04  0.677058  0.143654  0.179287\n",
      "4     BABA    20Aug05  0.735515  0.131579  0.132906\n",
      "..     ...        ...       ...       ...       ...\n",
      "501   BABA    21Sep26  0.559959  0.126927  0.313114\n",
      "502   BABA    21Sep27  0.659376  0.100793  0.239831\n",
      "503   BABA    21Sep28  0.615119  0.127082  0.257799\n",
      "504   BABA    21Sep29  0.633783  0.097332  0.268884\n",
      "505   BABA    21Sep30  0.656489  0.083950  0.259561\n",
      "\n",
      "[506 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "for ticker in tick_list:\n",
    "    print(ticker)\n",
    "    reddit_sentis = reddit_df_dict[ticker][['ticker', 'YearMonDay', 'neutral', 'positive', 'negative']]\n",
    "    twitter_sentis = twitter_df_dict[ticker][['ticker', 'YearMonDay', 'neutral', 'positive', 'negative']]\n",
    "    \n",
    "#     print(reddit_sentis,twitter_sentis,pd.concat([reddit_sentis,twitter_sentis], axis=0,ignore_index=True))\n",
    "    combined_sentis = pd.concat([reddit_sentis,twitter_sentis], axis=0,ignore_index=True)\n",
    "    combined_sentis_grouped = combined_sentis.groupby(['ticker', 'YearMonDay'])[['neutral', 'positive', 'negative']].mean().reset_index()\n",
    "    combined_sentis_grouped.to_pickle(f\"C:\\\\Users\\\\Karthik\\\\Desktop\\\\Dissertation\\\\Final_dfs\\\\combined_{ticker}_BERT_TwitterAirline_Even.pkl\")\n",
    "    print(combined_sentis_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- https://machinelearningmastery.com/exploding-gradients-in-neural-networks/\n",
    "- https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
