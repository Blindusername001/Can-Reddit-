{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1B: Code to get comments from Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-30 2020-06-01\n"
     ]
    }
   ],
   "source": [
    "base = datetime.datetime(2021,9,30)\n",
    "date_list = [(base - datetime.timedelta(days=x)).strftime('%Y-%m-%d') for x in range(487)]\n",
    "print(date_list[0], date_list[len(date_list)-1])\n",
    "# date_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract data from twitter\n",
    "- *Twitter data was extracted through a library called snsscrape*\n",
    "- *snsscrape is a CLI module; Hence the use of 'os' library to send command to CLI promt to scrape required data from twitter*\n",
    "- *Since twitter does not have specific threads such as reddit to focus on, the number of search results were limited to 1000 everyday*\n",
    "- *Due to the long running nature of the below block, there were a few disrruptions due to power/ connectivity issues. So the block had to be manipulated a few times to pull data for all tickers*\n",
    "- *The tickers were selected based on the tickers available in reddit comments*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick_list =  ['AMC', 'DKNG', 'TSLA', 'AMD', 'BABA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_count = 1000\n",
    "\n",
    "for tick in tick_list:\n",
    "    text_query = f\"#{tick} OR ${tick}\"\n",
    "#     print(text_query)\n",
    "    date_block_start = date_list[0] \n",
    "    date_block_end = date_list[len(date_list)-1]\n",
    "\n",
    "    for dt in date_list:\n",
    "        td = (datetime.datetime.strptime(dt,'%Y-%m-%d')+datetime.timedelta(days=1)).date()\n",
    "        os.system('snscrape --jsonl --max-results {} --since {} twitter-search \"{} until:{}\"> text-query-tweets.json'.format(tweet_count, dt, text_query, td))\n",
    "        tweets_df = pd.read_json('text-query-tweets.json', lines=True)\n",
    "        if dt == date_list[0]:\n",
    "            tweets_df.to_csv(f'C:\\\\Users\\\\Karthik\\\\Desktop\\\\Dissertation\\\\Twitter\\\\tweets\\\\text-query-tweets-{tick}-{date_block_start}_{date_block_end}.csv', sep=',', index=False, mode='w', header=True)   \n",
    "        else:\n",
    "            tweets_df.to_csv(f'C:\\\\Users\\\\Karthik\\\\Desktop\\\\Dissertation\\\\Twitter\\\\tweets\\\\text-query-tweets-{tick}-{date_block_start}_{date_block_end}.csv', sep=',', index=False, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# tick_list =  ['BABA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# tweet_count = 1000\n",
    "\n",
    "# for tick in tick_list:\n",
    "#     text_query = f\"#{tick} OR ${tick}\"\n",
    "# #     print(text_query)\n",
    "#     date_block_start = date_list[0] \n",
    "#     date_block_end = date_list[len(date_list)-1]\n",
    "\n",
    "#     for dt in date_list:\n",
    "#         td = (datetime.datetime.strptime(dt,'%Y-%m-%d')+datetime.timedelta(days=1)).date()\n",
    "#         os.system('snscrape --jsonl --max-results {} --since {} twitter-search \"{} until:{}\"> text-query-tweets.json'.format(tweet_count, dt, text_query, td))\n",
    "#         tweets_df = pd.read_json('text-query-tweets.json', lines=True)\n",
    "#         if dt == date_list[0]:\n",
    "#             tweets_df.to_csv(f'C:\\\\Users\\\\Karthik\\\\Desktop\\\\Dissertation\\\\Twitter\\\\tweets\\\\text-query-tweets-{tick}-{date_block_start}_{date_block_end}.csv', sep=',', index=False, mode='w', header=True)   \n",
    "#         else:\n",
    "#             tweets_df.to_csv(f'C:\\\\Users\\\\Karthik\\\\Desktop\\\\Dissertation\\\\Twitter\\\\tweets\\\\text-query-tweets-{tick}-{date_block_start}_{date_block_end}.csv', sep=',', index=False, mode='a', header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
